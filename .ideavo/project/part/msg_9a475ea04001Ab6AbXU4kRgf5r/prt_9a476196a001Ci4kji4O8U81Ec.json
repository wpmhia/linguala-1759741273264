{
  "id": "prt_9a476196a001Ci4kji4O8U81Ec",
  "messageID": "msg_9a475ea04001Ab6AbXU4kRgf5r",
  "sessionID": "ses_65b8a18f2ffeMlgQ4UP0yHPHsm",
  "type": "tool",
  "tool": "webfetch",
  "callID": "toolu_01EsAsvNwFh6QAijhH27VTFF",
  "state": {
    "status": "completed",
    "input": {
      "url": "https://help.aliyun.com/zh/dashscope/developer-reference/api-details",
      "format": "markdown"
    },
    "output": "                  通义千问API参考\\_大模型服务平台百炼(Model Studio)-阿里云帮助中心               \n\n[](https://www.aliyun.com/)\n\n[大模型](https://www.aliyun.com/product/tongyi)[产品](https://www.aliyun.com/product/list)[解决方案](https://www.aliyun.com/solution/tech-solution/)[文档与社区](https://www.aliyun.com/resources)[权益中心](https://www.aliyun.com/benefit)[定价](https://www.aliyun.com/price)[云市场](https://market.aliyun.com/)[合作伙伴](https://partner.aliyun.com/management/v2)[支持与服务](https://www.aliyun.com/service)[了解阿里云*HOT*](https://www.aliyun.com/about)\n\n查看 “\n\n” 全部搜索结果\n\n[](https://www.aliyun.com/search?from=h5-global-nav-search)\n\n[\n\nAI 助理\n\n\n\n\n\n](https://www.aliyun.com/ai-assistant?displayMode=side)[备案](https://beian.aliyun.com/)[控制台](https://home.console.aliyun.com/home/dashboard/ProductAndService)\n\n[官方文档](/)\n\n输入文档关键字查找\n\n[大模型服务平台百炼](/zh/model-studio/)\n\n-   [用户指南（模型）](/zh/model-studio/model-user-guide/)\n-   [用户指南（应用）](/zh/model-studio/application-user-guide/)\n-   [API参考（模型）](/zh/model-studio/model-api-reference/)\n-   [API参考（应用）](/zh/model-studio/applicantion-api-reference/)\n-   [历史文档](/zh/model-studio/deprecated-features/)\n\n[首页](/) [大模型服务平台百炼](/zh/model-studio/) [API参考（模型）](/zh/model-studio/model-api-reference/) [对话](/zh/model-studio/chat/) 通义千问\n\n# 通义千问API参考\n\n更新时间：\n\n[产品详情](https://www.aliyun.com/product/bailian)\n\n[我的收藏](/my_favorites.html)\n\n本文介绍通义千问 API 的输入输出参数。\n\n> 模型介绍、选型建议和使用方法**，**请参考[文本生成模型概述](https://help.aliyun.com/zh/model-studio/text-generation)。\n\n您可以通过 OpenAI 兼容或 DashScope 的方式调用通义千问 API。\n\n## OpenAI 兼容\n\n## 北京地域\n\n使用SDK调用时需配置的base\\_url：`https://dashscope.aliyuncs.com/compatible-mode/v1`\n\n使用HTTP方式调用时需配置的endpoint：`POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions`\n\n## 新加坡地域\n\n使用SDK调用时需配置的base\\_url：`https://dashscope-intl.aliyuncs.com/compatible-mode/v1`\n\n使用HTTP方式调用时需配置的endpoint：`POST https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions`\n\n## 金融云\n\n使用SDK调用时需配置的base\\_url：`https://dashscope-finance.aliyuncs.com/compatible-mode/v1`\n\n使用HTTP方式调用时需配置的endpoint：`POST https://dashscope-finance.aliyuncs.com/compatible-mode/v1/chat/completions`\n\n> 您需要已[获取API Key](https://help.aliyun.com/zh/model-studio/get-api-key)并[配置API Key到环境变量](https://help.aliyun.com/zh/model-studio/configure-api-key-through-environment-variables)。如果通过OpenAI SDK进行调用，还需要[安装SDK](https://help.aliyun.com/zh/model-studio/install-sdk)。\n\n### 请求体\n\n## 文本输入\n\n> 此处以单轮对话作为示例，您也可以进行[多轮对话](https://help.aliyun.com/zh/model-studio/multi-round-conversation)。\n\n## Python\n\n```\nimport os\nfrom openai import OpenAI\n\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n\ncompletion = client.chat.completions.create(\n    # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    model=\"qwen-plus\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"你是谁？\"},\n    ],\n    # Qwen3模型通过enable_thinking参数控制思考过程（开源版默认True，商业版默认False）\n    # 使用Qwen3开源版模型时，若未启用流式输出，请将下行取消注释，否则会报错\n    # extra_body={\"enable_thinking\": False},\n)\nprint(completion.model_dump_json())\n```\n\n## Java\n\n```\n// 该代码 OpenAI SDK 版本为 2.6.0\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\n\npublic class Main {\n    public static void main(String[] args) {\n        OpenAIClient client = OpenAIOkHttpClient.builder()\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                .baseUrl(\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n                .build();\n\n        ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n                .addUserMessage(\"你是谁\")\n                .model(\"qwen-plus\")\n                .build();\n\n        try {\n            ChatCompletion chatCompletion = client.chat().completions().create(params);\n            System.out.println(chatCompletion);\n        } catch (Exception e) {\n            System.err.println(\"Error occurred: \" + e.getMessage());\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n## Node.js\n\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI(\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n        apiKey: process.env.DASHSCOPE_API_KEY,\n        baseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    }\n);\n\nasync function main() {\n    const completion = await openai.chat.completions.create({\n        model: \"qwen-plus\",  //此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        messages: [\n            { role: \"system\", content: \"You are a helpful assistant.\" },\n            { role: \"user\", content: \"你是谁？\" }\n        ],\n    });\n    console.log(JSON.stringify(completion))\n}\n\nmain();\n```\n\n## Go\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"os\"\n\n\t\"github.com/openai/openai-go\"\n\t\"github.com/openai/openai-go/option\"\n)\n\nfunc main() {\n\tclient := openai.NewClient(\n\t\toption.WithAPIKey(os.Getenv(\"DASHSCOPE_API_KEY\")),\n\t\toption.WithBaseURL(\"https://dashscope.aliyuncs.com/compatible-mode/v1/\"),\n\t)\n\tchatCompletion, err := client.Chat.Completions.New(\n\t\tcontext.TODO(), openai.ChatCompletionNewParams{\n\t\t\tMessages: openai.F(\n\t\t\t\t[]openai.ChatCompletionMessageParamUnion{\n\t\t\t\t\topenai.UserMessage(\"你是谁\"),\n\t\t\t\t},\n\t\t\t),\n\t\t\tModel: openai.F(\"qwen-plus\"),\n\t\t},\n\t)\n\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tprintln(chatCompletion.Choices[0].Message.Content)\n}\n```\n\n## C#（HTTP）\n\n```\nusing System.Net.Http.Headers;\nusing System.Text;\n\nclass Program\n{\n    private static readonly HttpClient httpClient = new HttpClient();\n\n    static async Task Main(string[] args)\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：string? apiKey = \"sk-xxx\";\n        string? apiKey = Environment.GetEnvironmentVariable(\"DASHSCOPE_API_KEY\");\n\n        if (string.IsNullOrEmpty(apiKey))\n        {\n            Console.WriteLine(\"API Key 未设置。请确保环境变量 'DASHSCOPE_API_KEY' 已设置。\");\n            return;\n        }\n\n        // 设置请求 URL 和内容\n        string url = \"https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions\";\n        // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        string jsonContent = @\"{\n            \"\"model\"\": \"\"qwen-plus\"\",\n            \"\"messages\"\": [\n                {\n                    \"\"role\"\": \"\"system\"\",\n                    \"\"content\"\": \"\"You are a helpful assistant.\"\"\n                },\n                {\n                    \"\"role\"\": \"\"user\"\", \n                    \"\"content\"\": \"\"你是谁？\"\"\n                }\n            ]\n        }\";\n\n        // 发送请求并获取响应\n        string result = await SendPostRequestAsync(url, jsonContent, apiKey);\n\n        // 输出结果\n        Console.WriteLine(result);\n    }\n\n    private static async Task<string> SendPostRequestAsync(string url, string jsonContent, string apiKey)\n    {\n        using (var content = new StringContent(jsonContent, Encoding.UTF8, \"application/json\"))\n        {\n            // 设置请求头\n            httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\"));\n\n            // 发送请求并获取响应\n            HttpResponseMessage response = await httpClient.PostAsync(url, content);\n\n            // 处理响应\n            if (response.IsSuccessStatusCode)\n            {\n                return await response.Content.ReadAsStringAsync();\n            }\n            else\n            {\n                return $\"请求失败: {response.StatusCode}\";\n            }\n        }\n    }\n}\n```\n\n## PHP（HTTP）\n\n```\n<?php\n// 设置请求的URL\n$url = 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions';\n// 若没有配置环境变量，请用百炼API Key将下行替换为：$apiKey = \"sk-xxx\";\n$apiKey = getenv('DASHSCOPE_API_KEY');\n// 设置请求头\n$headers = [\n    'Authorization: Bearer '.$apiKey,\n    'Content-Type: application/json'\n];\n// 设置请求体\n$data = [\n    // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    \"model\" => \"qwen-plus\",\n    \"messages\" => [\n        [\n            \"role\" => \"system\",\n            \"content\" => \"You are a helpful assistant.\"\n        ],\n        [\n            \"role\" => \"user\",\n            \"content\" => \"你是谁？\"\n        ]\n    ]\n];\n// 初始化cURL会话\n$ch = curl_init();\n// 设置cURL选项\ncurl_setopt($ch, CURLOPT_URL, $url);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, $headers);\n// 执行cURL会话\n$response = curl_exec($ch);\n// 检查是否有错误发生\nif (curl_errno($ch)) {\n    echo 'Curl error: ' . curl_error($ch);\n}\n// 关闭cURL资源\ncurl_close($ch);\n// 输出响应结果\necho $response;\n?>\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"qwen-plus\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"你是谁？\"\n        }\n    ]\n}'\n```\n\n## 流式输出\n\n> 更多用法请参见[流式输出](https://help.aliyun.com/zh/model-studio/stream)。\n\n## Python\n\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\n    model=\"qwen-plus\",  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=[{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': '你是谁？'}],\n    stream=True,\n    stream_options={\"include_usage\": True}\n    )\nfor chunk in completion:\n    print(chunk.model_dump_json())\n```\n\n## Node.js\n\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI(\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n        apiKey: process.env.DASHSCOPE_API_KEY,\n        baseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    }\n);\n\nasync function main() {\n    const completion = await openai.chat.completions.create({\n        model: \"qwen-plus\", // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        messages: [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"你是谁？\"}\n        ],\n        stream: true,\n    });\n    for await (const chunk of completion) {\n        console.log(JSON.stringify(chunk));\n    }\n}\n\nmain();\n```\n\n## curl\n\n```\ncurl --location \"https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model\": \"qwen-plus\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"你是谁？\"\n        }\n    ],\n    \"stream\":true\n}'\n```\n\n## 图像输入\n\n> 关于大模型分析图像的更多用法，请参见[视觉理解](https://help.aliyun.com/zh/model-studio/vision)。\n\n## Python\n\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\n    model=\"qwen-vl-plus\",  # 此处以qwen-vl-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=[{\"role\": \"user\",\"content\": [\n            {\"type\": \"image_url\",\n             \"image_url\": {\"url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"}},\n            {\"type\": \"text\", \"text\": \"这是什么\"},\n            ]}]\n    )\nprint(completion.model_dump_json())\n```\n\n## Node.js\n\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI(\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n        apiKey: process.env.DASHSCOPE_API_KEY,\n        baseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    }\n);\n\nasync function main() {\n    const response = await openai.chat.completions.create({\n        model: \"qwen-vl-max\", // 此处以qwen-vl-max为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        messages: [{role: \"user\",content: [\n            { type: \"image_url\",image_url: {\"url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"}},\n            { type: \"text\", text: \"这是什么？\" },\n        ]}]\n    });\n    console.log(JSON.stringify(response));\n}\n\nmain();\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n  \"model\": \"qwen-vl-plus\",\n  \"messages\": [{\n      \"role\": \"user\",\n      \"content\": [\n       {\"type\": \"image_url\",\"image_url\": {\"url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"}},\n       {\"type\": \"text\",\"text\": \"这是什么\"}\n       ]}]\n}'\n```\n\n## 视频输入\n\n> 以下为传入图片列表的示例代码，关于更多用法（如传入视频文件），请参见[视觉理解](https://help.aliyun.com/zh/model-studio/vision#80dbf6ca8fh6s)。\n\n## Python\n\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\n    # 此处以qwen-vl-max-latest为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    model=\"qwen-vl-max-latest\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n                    \"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n                    \"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n                    \"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"]\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"描述这个视频的具体过程\"\n            }]}]\n)\nprint(completion.model_dump_json())\n```\n\n## Node.js\n\n```\n// 确保之前在 package.json 中指定了 \"type\": \"module\"\nimport OpenAI from \"openai\"; \n\nconst openai = new OpenAI({\n    // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n    apiKey: process.env.DASHSCOPE_API_KEY, \n    baseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n});\n\nasync function main() {\n    const response = await openai.chat.completions.create({\n        // 此处以qwen-vl-max-latest为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models \n        model: \"qwen-vl-max-latest\",\n        messages: [{\n            role: \"user\",\n            content: [\n                {\n                    type: \"video\",\n                    video: [\n                        \"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n                        \"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n                        \"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n                        \"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"\n                    ]\n                },\n                {\n                    type: \"text\",\n                    text: \"描述这个视频的具体过程\"\n                }\n        ]}]\n    });\n    console.log(JSON.stringify(response));\n}\n\nmain();\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model\": \"qwen-vl-max-latest\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"video\",\n                    \"video\": [\n                        \"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n                        \"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n                        \"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n                        \"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"\n                    ]\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"描述这个视频的具体过程\"\n                }\n            ]\n        }\n    ]\n}'\n```\n\n## 工具调用\n\n> 完整的Function Calling流程代码请参见[Function Calling](https://help.aliyun.com/zh/model-studio/qwen-function-calling)。\n\n> Qwen3（思考模式）与QwQ 模型的 Function Calling 代码请参见[工具调用](https://help.aliyun.com/zh/model-studio/deep-thinking#c858c412ecx9i)。\n\n## Python\n\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope SDK的base_url\n)\n\ntools = [\n    # 工具1 获取当前时刻的时间\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_time\",\n            \"description\": \"当你想知道现在的时间时非常有用。\",\n            \"parameters\": {}  # 因为获取当前时间无需输入参数，因此parameters为空字典\n        }\n    },  \n    # 工具2 获取指定城市的天气\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"当你想查询指定城市的天气时非常有用。\",\n            \"parameters\": {  \n                \"type\": \"object\",\n                \"properties\": {\n                    # 查询天气时需要提供位置，因此参数设置为location\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }\n]\nmessages = [{\"role\": \"user\", \"content\": \"杭州天气怎么样\"}]\ncompletion = client.chat.completions.create(\n    model=\"qwen-plus\",  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages,\n    tools=tools\n)\n\nprint(completion.model_dump_json())\n```\n\n## Node.js\n\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI(\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n        apiKey: process.env.DASHSCOPE_API_KEY,\n        baseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    }\n);\n\nconst messages = [{\"role\": \"user\", \"content\": \"杭州天气怎么样\"}];\nconst tools = [\n// 工具1 获取当前时刻的时间\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_time\",\n        \"description\": \"当你想知道现在的时间时非常有用。\",\n        // 因为获取当前时间无需输入参数，因此parameters为空\n        \"parameters\": {}  \n    }\n},  \n// 工具2 获取指定城市的天气\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"当你想查询指定城市的天气时非常有用。\",\n        \"parameters\": {  \n            \"type\": \"object\",\n            \"properties\": {\n                // 查询天气时需要提供位置，因此参数设置为location\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    }\n}\n];\n\nasync function main() {\n    const response = await openai.chat.completions.create({\n        model: \"qwen-plus\", // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        messages: messages,\n        tools: tools,\n    });\n    console.log(JSON.stringify(response));\n}\n\nmain();\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"qwen-plus\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"杭州天气怎么样\"\n        }\n    ],\n    \"tools\": [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_time\",\n            \"description\": \"当你想知道现在的时间时非常有用。\",\n            \"parameters\": {}\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"当你想查询指定城市的天气时非常有用。\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\":{\n                        \"type\": \"string\",\n                        \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }\n  ]\n}'\n```\n\n## 联网搜索\n\n## Python\n\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), \n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope服务的base_url\n)\ncompletion = client.chat.completions.create(\n    model=\"qwen-plus\",  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=[\n        {'role': 'system', 'content': 'You are a helpful assistant.'},\n        {'role': 'user', 'content': '中国队在巴黎奥运会获得了多少枚金牌'}],\n    extra_body={\n        \"enable_search\": True\n    }\n    )\nprint(completion.model_dump_json())\n```\n\n## Node.js\n\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI(\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n        apiKey: process.env.DASHSCOPE_API_KEY,\n        baseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    }\n);\nasync function main() {\n    const completion = await openai.chat.completions.create({\n        model: \"qwen-plus\", //此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        messages: [\n            { role: \"system\", content: \"You are a helpful assistant.\" },\n            { role: \"user\", content: \"中国队在巴黎奥运会获得了多少枚金牌\" }\n        ],\n        enable_search:true\n    });\n    console.log(JSON.stringify(completion))\n}\n\nmain();\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"qwen-plus\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"中国队在巴黎奥运会获得了多少枚金牌\"\n        }\n    ],\n    \"enable_search\": true\n}'\n```\n\n## 异步调用\n\n```\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\nimport platform\n\nclient = AsyncOpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n\nasync def main():\n    response = await client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"你是谁\"}],\n        model=\"qwen-plus\",  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    )\n    print(response.model_dump_json())\n\nif platform.system() == \"Windows\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncio.run(main())\n```\n\n## 文档理解\n\n> 当前仅qwen-long模型支持对文档进行分析，详细用法请参见[长上下文（Qwen-Long）](https://help.aliyun.com/zh/model-studio/long-context-qwen-long)。\n\n## Python\n\n```\nimport os\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\nfile_object = client.files.create(file=Path(\"百炼系列手机产品介绍.docx\"), purpose=\"file-extract\")\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",  # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=[\n        {'role': 'system', 'content': f'fileid://{file_object.id}'},\n        {'role': 'user', 'content': '这篇文章讲了什么？'}\n    ]\n)\nprint(completion.model_dump_json())\n```\n\n## Java\n\n```\n// 建议OpenAI SDK的版本 >= 0.32.0\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.ChatCompletion;\nimport com.openai.models.ChatCompletionCreateParams;\nimport com.openai.models.FileCreateParams;\nimport com.openai.models.FileObject;\nimport com.openai.models.FilePurpose;\n\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\npublic class Main {\n    public static void main(String[] args) {\n        // 创建客户端，使用环境变量中的API密钥\n        OpenAIClient client = OpenAIOkHttpClient.builder()\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                .baseUrl(\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n                .build();\n\n        // 设置文件路径\n        Path filePath = Paths.get(\"百炼系列手机产品介绍.docx\");\n        // 创建文件上传参数\n        FileCreateParams fileParams = FileCreateParams.builder()\n                .file(filePath)\n                .purpose(FilePurpose.of(\"file-extract\"))\n                .build();\n\n        // 上传文件\n        FileObject fileObject = client.files().create(fileParams);\n        String fileId = fileObject.id();\n\n        // 创建聊天请求\n        ChatCompletionCreateParams chatParams = ChatCompletionCreateParams.builder()\n                .addSystemMessage(\"fileid://\" + fileId)\n                .addUserMessage(\"这篇文章讲了什么？\")\n                .model(\"qwen-long\") \n                .build();\n\n        // 发送请求并获取响应\n        ChatCompletion chatCompletion = client.chat().completions().create(chatParams);\n\n        // 打印响应结果\n        System.out.println(chatCompletion);\n    }\n}\n```\n\n## Node.js\n\n```\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI(\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n        apiKey: process.env.DASHSCOPE_API_KEY,\n        baseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    }\n);\n\nasync function getFileID() {\n    const fileObject = await openai.files.create({\n        file: fs.createReadStream(\"百炼系列手机产品介绍.docx\"),\n        purpose: \"file-extract\"\n    });\n    return fileObject.id;\n}   \n\nasync function main() {\n    const fileID = await getFileID();\n    const completion = await openai.chat.completions.create({\n        model: \"qwen-long\",  //模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        messages: [\n            { role: \"system\", content: `fileid://${fileID}`},\n            { role: \"user\", content: \"这篇文章讲了什么？\" }\n        ],\n    });\n    console.log(JSON.stringify(completion))\n}\n\nmain();\n```\n\n## curl\n\n```\ncurl --location 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model\": \"qwen-long\",\n    \"messages\": [\n        {\"role\": \"system\",\"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"system\",\"content\": \"fileid://file-fe-xxx\"},\n        {\"role\": \"user\",\"content\": \"这篇文章讲了什么？\"}\n    ],\n    \"stream\": true,\n    \"stream_options\": {\n        \"include_usage\": true\n    }\n}'\n```\n\n## 文字提取\n\n> 关于通义千问OCR模型进行文字提取更多用法，请参见[文字提取](https://help.aliyun.com/zh/model-studio/qwen-vl-ocr)。\n\n## Python\n\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n# 设置抽取的字段和格式\nresult_schema = \"\"\"\n        {\n          \"销售方名称\": \"\",\n          \"购买方名称\": \"\",\n          \"不含税价\": \"\",\n          \"组织机构代码\": \"\",\n          \"发票代码\": \"\"\n        }\n        \"\"\"\n# 拼接Prompt \nprompt = f\"\"\"假设你是一名信息提取专家。现在给你一个JSON模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的JSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用null填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的JSON模式内容如下: {result_schema}。\"\"\"\n\ncompletion = client.chat.completions.create(\n    model=\"qwen-vl-ocr-latest\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n                    # 输入图像的最小像素阈值，小于该值图像会按原比例放大，直到总像素大于min_pixels\n                    \"min_pixels\": 28 * 28 * 4,\n                    # 输入图像的最大像素阈值，超过该值图像会按原比例缩小，直到总像素低于max_pixels\n                    \"max_pixels\": 28 * 28 * 8192\n                },\n                # 使用任务指定的Prompt\n                {\"type\": \"text\", \"text\": prompt},\n            ]\n        }\n    ])\n\nprint(completion.choices[0].message.content)\n```\n\n## Node.js\n\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  // 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey: \"sk-xxx\",\n  apiKey: process.env.DASHSCOPE_API_KEY,\n  baseURL: 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n});\n// 设置抽取的字段和格式\nconst resultSchema = `{\n          \"销售方名称\": \"\",\n          \"购买方名称\": \"\",\n          \"不含税价\": \"\",\n          \"组织机构代码\": \"\",\n          \"发票代码\": \"\"\n        }`;\n// 拼接Prompt\nconst prompt = `假设你是一名信息提取专家。现在给你一个JSON模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的JSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用null填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的JSON模式内容如下: ${resultSchema}`;\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    model: 'qwen-vl-ocr-latest',\n    messages: [\n      {\n        role: 'user',\n        content: [\n           // 可自定义Prompt，若未设置则使用默认的Prompt\n          { type: 'text', text: prompt},\n          {\n            type: 'image_url',\n            image_url: {\n              url: 'https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg',\n            },\n              //  输入图像的最小像素阈值，小于该值图像会按原比例放大，直到总像素大于min_pixels\n              \"min_pixels\": 28 * 28 * 4,\n              // 输入图像的最大像素阈值，超过该值图像会按原比例缩小，直到总像素低于max_pixels\n              \"max_pixels\": 28 * 28 * 8192\n          }\n        ]\n      }\n    ]\n  });\n  console.log(response.choices[0].message.content);\n}\n\nmain();\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"model\": \"qwen-vl-ocr-latest\",\n  \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n                    \"min_pixels\": 3136,\n                    \"max_pixels\": 6422528\n                },\n                {\"type\": \"text\", \"text\": \"假设你是一名信息提取专家。现在给你一个JSON模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的JSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用null填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的JSON模式内容如下::{\\\"销售方名称\\\": \\\"\\\",\\\"购买方名称\\\": \\\"\\\",\\\"不含税价\\\": \\\"\\\",\\\"组织机构代码\\\": \\\"\\\",\\\"发票代码\\\": \\\"\\\"}\"}\n            ]\n        }\n    ]\n}'\n```\n\n**model** `*string*` **（必选）**\n\n模型名称。\n\n支持的模型：通义千问大语言模型（商业版、开源版）、通义千问VL、代码模型、通义千问Omni、数学模型。\n\n> 通义千问Audio暂不支持OpenAI兼容模式，仅支持[DashScope](#a9b7b197e2q2v)方式。\n\n**具体模型名称和计费，请参见**[模型列表](https://help.aliyun.com/zh/model-studio/models#9f8890ce29g5u)。\n\n**messages** `*array*` **（必选）**\n\n由历史对话组成的消息列表。\n\n**消息类型**\n\nSystem Message `*object*` （可选）\n\n模型的目标或角色。如果设置系统消息，请放在messages列表的第一位。\n\n**属性**\n\n**content** `*string*` **（必选）**\n\n消息内容。\n\n**role** `*string*` **（必选）**\n\n固定为`system`。\n\n> QwQ 、Qwen3-VL模型不建议设置 System Message，QVQ 模型设置System Message不会生效。\n\nUser Message `*object*` **（必选）**\n\n用户发送给模型的消息。\n\n**属性**\n\n**content** `*string 或 array*` **（必选）**\n\n消息内容。如果您的输入只有文本，则为 string 类型；如果您的输入包含图像等多模态数据，则为 array 类型。\n\n> 如需传入音频给通义千问Audio模型，请前往[DashScope](#69cac67a477k2)查看，暂不支持使用OpenAI兼容的方式。\n\n**使用多模态模型时的属性**\n\n**type** `*string*` **（必选）**\n\n可取值：\n\n-   `\"text\"`\n    \n    向[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)或[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)输入文本时需要设为`\"text\"`。\n    \n-   `\"image_url\"`\n    \n    向[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)或[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)输入图片时需要设为`\"image_url\"`。\n    \n-   `\"input_audio\"`\n    \n    向[Qwen-Omni模型](https://help.aliyun.com/zh/model-studio/qwen-omni)、[Qwen3-Omn-Captioner 模型](https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner)输入音频时需要设为`\"input_audio\"`。\n    \n-   `\"video\"`\n    \n    向[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)或[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)输入图片列表形式的视频时需要设为`\"video\"`。\n    \n-   `\"video_url\"`\n    \n    向[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)或[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)输入视频文件时需要设为`\"video_url\"`。\n    \n\n> 对于Qwen-VL模型，仅部分模型可直接传入视频文件，详情请参见[视频理解（Qwen-VL）](https://help.aliyun.com/zh/model-studio/vision#80dbf6ca8fh6s)；[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)或[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)支持直接传入视频文件。\n\n**text** `*string*`\n\n> 当`type`为`\"text\"`时，是必选参数。\n\n输入的文本。\n\n**image\\_url** `*object*`\n\n> 当`type`为`\"image_url\"`时，是必选参数。\n\n输入的图像信息。示例值：\n\n```\n{\n    \"url\": \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241022/emyrja/dog_and_girl.jpeg\"\n}\n```\n\n**属性**\n\n**url** `*string*`**（必选）**\n\n图片的 URL或 Base64 Data URL。传入本地文件请参考[视觉理解](https://help.aliyun.com/zh/model-studio/vision#647c6397db430)。\n\n**input\\_audio** `*object*`\n\n> 当使用[Qwen-Omni](https://help.aliyun.com/zh/model-studio/qwen-omni)、[Qwen3-Omn-Captioner](https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner) 模型，且当`type`为`\"input_audio\"`时，是必选参数。\n\n输入的音频信息。示例值：\n\n```\n{\n    \"data\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\",\n    \"format\": \"mp3\"\n}\n```\n\n**属性**\n\n**data** `*string*`**（必选）**\n\n音频的 URL 或Base64 Data URL。传入本地文件请参见：[输入 Base64 编码的本地文件](https://help.aliyun.com/zh/model-studio/qwen-omni#c516d1e824x03)。\n\n**format** `*string*`**（必选）**\n\n输入音频的格式，如\"mp3\"、\"wav\"等。\n\n**video** `*array*`\n\n输入的图片列表形式的视频信息。使用方法请参见：[视频理解（Qwen-VL）](https://help.aliyun.com/zh/model-studio/vision#80dbf6ca8fh6s)、[视频理解（QVQ）](https://help.aliyun.com/zh/model-studio/qvq#e6df293d5565g)或[视频理解（Qwen-Omni）](https://help.aliyun.com/zh/model-studio/qwen-omni#0f4360d63a8nk)。\n\n示例值：\n\n```\n[\n    \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/xzsgiz/football1.jpg\",\n    \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/tdescd/football2.jpg\",\n    \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/zefdja/football3.jpg\",\n    \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/aedbqh/football4.jpg\"\n]\n```\n\n> 当使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)或[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)，且`type`参数为`\"video\"`时是必选参数。\n\n**video\\_url** `*object*`\n\n输入的视频文件信息。\n\n示例值：\n\n```\n{\n    \"url\": \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241115/cqqkru/1.mp4\"\n}\n```\n\n**属性**\n\n**url** `*string*`**（必选）**\n\n视频文件的公网 URL 或 Base64 Data URL。向 Qwen-Omni 模型输入本地视频文件请参见[输入 Base64 编码的本地文件](https://help.aliyun.com/zh/model-studio/qwen-omni#c516d1e824x03)。\n\n> 当使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)或[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)，且`type`参数为`\"video_url\"`时是必选参数。\n\n> 对于Qwen-VL 模型，仅部分模型可直接传入视频文件，详情请参见[视频理解（Qwen-VL）](https://help.aliyun.com/zh/model-studio/vision#80dbf6ca8fh6s)；对于QVQ和Qwen-Omni 模型，可直接传入视频文件。\n\n> Qwen-Omni 模型可以理解视频文件中的视觉与音频信息。\n\n**min\\_pixels** `*integer*` **（可选）**\n\nQwen-OCR、Qwen-VL模型支持，用于设定输入图像的最小像素阈值。\n\n当输入图像像素小于`min_pixels`时，会将图像按原比例放大，直到总像素高于`min_pixels`。\n\n**min\\_pixels 取值范围**\n\n-   Qwen-OCR、`qwen-vl-max-0813及之前`、`qwen-vl-plus-0710`及之前更新的模型：默认值和最小值均为3136\n    \n-   `qwen-vl-max-0813`及之后、`qwen-vl-plus-0710`及以后更新的模型：默认值和最小值均为4096\n    \n-   Qwen3-VL：默认值：65536，最小值：4096\n    \n\n**max\\_pixels** `*integer*` **（可选）**\n\nQwen-OCR、Qwen-VL支持，用于设定输入图像的最大像素阈值。\n\n当输入图像像素在`[min_pixels, max_pixels]`区间内时，模型会按原图进行识别。当输入图像像素大于`max_pixels`时，会将图像按原比例缩小，直到总像素低于`max_pixels`。\n\n**max\\_pixels 取值范围**\n\n-   对于Qwen-OCR模型：默认值：6422528，最大值：23520000\n    \n-   对于Qwen-VL模型，分两种情况：\n    \n    -   当 [vl\\_high\\_resolution\\_images](#0edad44583knr)为False（关闭高分辨率模式）时：\n        \n        -   Qwen2.5-VL：默认值和最大值均为1003520\n            \n        -   Qwen3-VL：默认值和最大值均为：2621440\n            \n    -   当[vl\\_high\\_resolution\\_images](#0edad44583knr)为True（开启高分辨率模式）时：\n        \n        -   Qwen2.5-VL：固定为 12845056\n            \n        -   Qwen3-VL：固定为 16777216\n            \n\n**cache\\_control** `*object*` **（可选）**\n\n仅支持[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)的模型支持，用于开启显式缓存。\n\n**属性**\n\n**type** `*string*`**（必选）**\n\n固定为`ephemeral`。\n\n**role** `*string*` **（必选）**\n\n固定为`user`。\n\nAssistant Message `*object*` （可选）\n\n模型对用户消息的回复。\n\n**属性**\n\n**content** `*string*` （可选）\n\n消息内容。仅当助手消息中指定`tool_calls`参数时非必选。\n\n**role** `*string*` **（必选）**\n\n固定为`assistant`。\n\n**partial** `*boolean*` （可选）\n\n是否开启Partial Mode。使用方法请参考[前缀续写](https://help.aliyun.com/zh/model-studio/partial-mode)。\n\n**支持的模型**\n\n-   **通义千问Max 系列**\n    \n    qwen-max、qwen-max-latest、qwen-max-2024-09-19及之后的快照模型\n    \n-   **通义千问Plus 系列（非思考模式）**\n    \n    qwen-plus、qwen-plus-latest、qwen-plus-2024-09-19及之后的快照模型\n    \n-   **通义千问Flash 系列（非思考模式）**\n    \n    qwen-flash、qwen-flash-2025-07-28及之后的快照模型\n    \n-   **通义千问Coder 系列**\n    \n    qwen3-coder-plus、qwen3-coder-flash、qwen3-coder-480b-a35b-instruct、qwen3-coder-30b-a3b-instruct、qwen-coder-plus、qwen-coder-plus-latest、qwen-coder-plus-2024-11-06、qwen-coder-turbo、qwen-coder-turbo-latest、qwen-coder-turbo-2024-09-19、qwen2.5-coder-32b-instruct、qwen2.5-coder-14b-instruct、qwen2.5-coder-7b-instruct、qwen2.5-coder-3b-instruct、qwen2.5-coder-1.5b-instruct、qwen2.5-coder-0.5b-instruct\n    \n-   **通义千问VL 系列**\n    \n    -   **qwen-vl-max 系列**\n        \n        qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2024-08-09及之后的快照模型\n        \n    -   **qwen-vl-plus 系列**\n        \n        qwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2024-08-09及之后的快照模型\n        \n-   **通义千问Turbo 系列（非思考模式）**\n    \n    qwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19及之后的快照模型\n    \n-   **通义千问开源系列**\n    \n    Qwen3 开源模型（非思考模式）、qwen2.5-72b-instruct、qwen2.5-32b-instruct、qwen2.5-14b-instruct、qwen2.5-7b-instruct、qwen2.5-3b-instruct、qwen2.5-1.5b-instruct、qwen2.5-0.5b-instruct\n    \n-   **通义千问Math 系列**\n    \n    qwen-math-plus、qwen-math-plus-latest、qwen-math-plus-0919、qwen-math-turbo、qwen-math-turbo-latest、qwen-math-turbo-0919、qwen2.5-math-72b-instruct、qwen2.5-math-7b-instruct、qwen2.5-math-1.5b-instruct\n    \n\n**tool\\_calls** `*array*` （可选）\n\n在发起 Function Calling后，模型回复的要调用的工具和调用工具时需要的参数。包含一个或多个对象。由上一轮模型响应的`tool_calls`字段获得。\n\n**属性**\n\n**id** `*string*`\n\n本次工具响应的ID。\n\n**type** `*string*`\n\n工具的类型，当前只支持`function`。\n\n**function** `*object*`\n\n需要被调用的函数。\n\n**属性**\n\n**name** `*string*`\n\n需要被调用的函数名。\n\n**arguments** `*string*`\n\n需要输入到工具中的参数，为JSON字符串。\n\n**index** `*integer*`\n\n工具信息在`tool_calls`列表中的索引。\n\nTool Message `*object*` （可选）\n\n工具的输出信息。\n\n**属性**\n\n**content** `*string*` **（必选）**\n\n消息内容，一般为工具函数的输出。\n\n**role** `*string*` **（必选）**\n\n固定为`tool`。\n\n**tool\\_call\\_id** `*string*` **（可选）**\n\n发起 Function Calling 后返回的 id，可以通过`completion.choices[0].message.tool_calls[0].id`获取，用于标记 Tool Message 对应的工具。\n\n**stream** `*boolean*` （可选） 默认值为 `false`\n\n是否流式输出回复。参数值：\n\n-   `false`：模型生成完所有内容后一次性返回结果。\n    \n-   `true`：边生成边输出，即每生成一部分内容就立即输出一个片段（chunk）。您需要实时地逐个读取这些片段以获得完整的结果。\n    \n\n> Qwen3商业版（思考模式）、Qwen3开源版、QwQ、QVQ只支持流式输出。\n\n**stream\\_options** `*object*` （可选）\n\n当启用流式输出时，可通过将本参数设置为`{\"include_usage\": true}`，在输出的最后一行显示所使用的Token数。\n\n> 如果设置为false，则最后一行不显示使用的Token数。\n\n本参数仅在设置stream为true时生效。\n\n**modalities** `array` （可选）默认值为`[\"text\"]`\n\n输出数据的模态，仅支持 [Qwen-Omni](https://help.aliyun.com/zh/model-studio/qwen-omni) 模型指定。可选值：\n\n-   `[\"text\",\"audio\"]`：输出文本与音频；\n    \n-   `[\"text\"]`：输出文本。\n    \n\n**audio** `*object*` （可选）\n\n输出音频的音色与格式，仅支持 [Qwen-Omni](https://help.aliyun.com/zh/model-studio/qwen-omni) 模型，且`modalities`参数需要包含`\"audio\"`。\n\n**属性**\n\n**voice** `*string*` （必选）\n\n输出音频的音色，可选值：\n\n-   Cherry\n    \n-   Serena\n    \n-   Ethan\n    \n-   Chelsie\n    \n\n音色效果请参见：[全模态](https://help.aliyun.com/zh/model-studio/qwen-omni#ffc9e5119cdht)。\n\n**format** `*string*` （必选）\n\n输出音频的格式，当前仅支持设定为`\"wav\"`。\n\n**temperature** `*float*` （可选）\n\n采样温度，控制模型生成文本的多样性。\n\ntemperature越高，生成的文本更多样，反之，生成的文本更确定。\n\n取值范围： \\[0, 2)\n\n由于temperature与top\\_p均可以控制生成文本的多样性，因此建议您只设置其中一个值。更多说明，请参见[文本生成模型概述](https://help.aliyun.com/zh/model-studio/text-generation#ad7b336bec5fw)。\n\n**temperature默认值**\n\nQwen3（非思考模式）、Qwen3-Instruct系列、Qwen3-Coder系列、qwen-max系列、qwen-plus系列（非思考模式）、qwen-flash系列（非思考模式）、qwen-turbo系列（非思考模式）、qwen开源系列、qwen-coder系列、qwq-32b-preview、qwen-doc-turbo、qwen-vl-max-2025-08-13、Qwen3-VL（非思考模式）：0.7；\n\nqwen-long、qwen-omni-turbo系列：1.0；\n\nQVQ系列 、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15 : 0.5；\n\n其余qwen-vl系列、qwen-vl-ocr系列、qwen-omni-turbo系列、qvq-72b-preview：0.01；\n\nqwen-math系列：0；\n\nQwen3（思考模式）、Qwen3-Thinking、Qwen3-Omni-Captioner、QwQ 系列：0.6；\n\nqwen-plus-character：0.92\n\nqwen3-omni-flash系列：0.9\n\nQwen3-VL（思考模式）：0.8\n\n> 不建议修改QVQ模型的默认temperature值 。\n\n**top\\_p** `*float*` （可选）\n\n核采样的概率阈值，控制模型生成文本的多样性。\n\ntop\\_p越高，生成的文本更多样。反之，生成的文本更确定。\n\n取值范围：（0,1.0\\]\n\n由于temperature与top\\_p均可以控制生成文本的多样性，因此建议您只设置其中一个值。更多说明，请参见[文本生成模型概述](https://help.aliyun.com/zh/model-studio/text-generation#ad7b336bec5fw)。\n\n**top\\_p默认值**\n\nQwen3（非思考模式）、Qwen3-Instruct系列、Qwen3-Coder系列、qwen-max系列、qwen-plus系列（非思考模式）、qwen-flash系列（非思考模式）、qwen-turbo系列（非思考模式）、Qwen 2.5开源系列、qwen-coder系列、qwen-long、qwq-32b-preview、qwen-doc-turbo、qwen-vl-max-2025-08-13、Qwen3-VL（非思考）：0.8；\n\nqwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen2-vl-72b-instruct、qwen-omni-turbo 系列：0.01；\n\nqwen-vl-plus系列、qwen-vl-ocr系列、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08、qwen-vl-max-2025-04-02、qwen-vl-max-2025-01-25、qwen-vl-max-2024-12-30、qvq-72b-preview、qwen2-vl-2b-instruct、qwen2-vl-7b-instruct、qwen2.5-vl-3b-instruct、qwen2.5-vl-7b-instruct、qwen2.5-vl-32b-instruct、qwen2.5-vl-72b-instruct、qwen2.5-omni-7b：0.001；\n\nQVQ系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15 、qwen2-audio-instruct: 0.5；\n\nqwen-math系列、Qwen3-Omni-Flash系列：1.0；\n\nQwen3（思考模式）、Qwen3=VL（思考模式）、Qwen3-Thinking、QwQ 系列、Qwen3-Omni-Captioner、qwen-plus-character：0.95\n\n> 不建议修改QVQ模型的默认 top\\_p 值。\n\n**top\\_k** `*integer*` （可选）\n\n生成过程中采样候选集的大小。例如，取值为50时，仅将单次生成中得分最高的50个Token组成随机采样的候选集。取值越大，生成的随机性越高；取值越小，生成的确定性越高。取值为None或当top\\_k大于100时，表示不启用top\\_k策略，此时仅有top\\_p策略生效。\n\n取值需要大于或等于0。\n\n**top\\_k默认值**\n\nQVQ系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15：10；\n\nQwQ 系列：40；\n\nqwen-math 系列、其余qwen-vl-plus系列、qwen-vl-max-2025-08-13之前的模型、qwen-vl-ocr系列、qwen-audio-asr系列、qwen-audio-turbo系列、qwen2.5-omni-7b、qvq-72b-preview：1；\n\nQwen3-Omni-Flash系列：50\n\n其余模型均为20；\n\n> 通过 Python SDK调用时，请将 **top\\_k** 放入 **extra\\_body** 对象中，配置方式为：extra\\_body={\"top\\_k\":xxx}。\n\n> 不建议修改QVQ模型的默认 top\\_k 值。\n\n**presence\\_penalty** `*float*` （可选）\n\n控制模型生成文本时的内容重复度。\n\n取值范围：\\[-2.0, 2.0\\]。正数会减少重复度，负数会增加重复度。\n\n适用场景：\n\n较高的presence\\_penalty适用于要求多样性、趣味性或创造性的场景，如创意写作或头脑风暴。\n\n较低的presence\\_penalty适用于要求一致性或专业术语的场景，如技术文档或其他正式文档。\n\n**presence\\_penalty默认值**\n\nQwen3（非思考模式）、Qwen3-Instruct系列、qwen3-0.6b/1.7b/4b（思考模式）、QVQ系列、qwen-max、qwen-max-latest、qwen-max-latest、qwen-max-2024-09-19、qwen2.5-vl系列、qwen-vl-max系列、qwen-vl-plus、qwen2-vl-72b-instruct、qwen-vl-plus-2025-01-02、Qwen3-VL（非思考）：1.5；\n\nqwen-vl-plus-latest、qwen-vl-plus-2025-08-15、qwen-vl-plus-2025-07-10：1.2\n\nqwen-vl-plus-2025-01-25：1.0；\n\nqwen3-8b/14b/32b/30b-a3b/235b-a22b（思考模式）、qwen-plus/qwen-plus-latest/2025-04-28（思考模式）、qwen-turbo/qwen-turbo/2025-04-28（思考模式）：0.5；\n\n其余均为0.0。\n\n**原理介绍**\n\n如果参数值是正数，模型将对目前文本中已存在的Token施加一个惩罚值（惩罚值与文本出现的次数无关），减少这些Token重复出现的几率，从而减少内容重复度，增加用词多样性。\n\n**示例**\n\n提示词：把这句话翻译成中文“This movie is good. The plot is good, the acting is good, the music is good, and overall, the whole movie is just good. It is really good, in fact. The plot is so good, and the acting is so good, and the music is so good.”\n\n参数值为2.0：这部电影很好。剧情很棒，演技棒，音乐也非常好听，总的来说，整部电影都好得不得了。实际上它真的很优秀。剧情非常精彩，演技出色，音乐也是那么的动听。\n\n参数值为0.0：这部电影很好。剧情好，演技好，音乐也好，总的来说，整部电影都很好。事实上，它真的很棒。剧情非常好，演技也非常出色，音乐也同样优秀。\n\n参数值为-2.0：这部电影很好。情节很好，演技很好，音乐也很好，总的来说，整部电影都很好。实际上，它真的很棒。情节非常好，演技也非常好，音乐也非常好。\n\n> 使用qwen-vl-plus-2025-01-25模型进行文字提取时，建议设置presence\\_penalty为1.5。\n\n> 不建议修改QVQ模型的默认presence\\_penalty值。\n\n**response\\_format** `*object*` （可选） 默认值为`{\"type\": \"text\"}`\n\n返回内容的格式。可选值：`{\"type\": \"text\"}`或`{\"type\": \"json_object\"}`。设置为`{\"type\": \"json_object\"}`时会输出标准格式的JSON字符串。使用方法请参见：[结构化输出](https://help.aliyun.com/zh/model-studio/json-mode)。\n\n> 如果指定该参数为`{\"type\": \"json_object\"}`，您需要在System Message或User Message中指引模型输出JSON格式，如：“请按照json格式输出。”\n\n支持的模型\n\n-   **通义千问Max 系列：**qwen3-max、qwen3-max-2025-09-23、qwen3-amx-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19 及之后的快照模型\n    \n-   **通义千问Plus 系列（非思考模式）**：qwen-plus、qwen-plus-latest、qwen-plus-2024-09-19及之后的快照模型\n    \n-   **通义千问Flash 系列（非思考模式）：**qwen-flash、qwen-flash-2025-07-28及之后的快照模型\n    \n-   **通义千问Coder 系列**：qwen3-coder-plus、qwen3-coder-plus-2025-07-22、qwen3-coder-flash、qwen3-coder-flash-2025-07-28\n    \n-   **通义千问VL 系列：**qwen-vl-max、qwen-vl-plus（不包括最新版与快照版模型）\n    \n-   **通义千问Turbo 系列（非思考模式）**：qwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19及之后的快照模型\n    \n-   **Qwen 开源系列**\n    \n    -   Qwen3（非思考模式）\n        \n    -   Qwen3-Coder\n        \n    -   Qwen2.5 系列的文本模型（不含math与coder模型）\n        \n\n**max\\_input\\_tokens** `*integer*` （可选）\n\n允许输入的最大 Token 长度。目前仅支持qwen-plus-0728/latest模型。\n\n-   qwen-plus-latest 默认值：129,024\n    \n    > 后续默认值可能调整至1,000,000。\n    \n-   qwen-plus-2025-07-28 默认值：1,000,000\n    \n\n> 通过 Python SDK 调用时，请通过`extra_body`配置。配置方式为：`extra_body={\"max_input_tokens\": xxx}`。\n\n**max\\_tokens** `*integer*` （可选）\n\n本次请求返回的最大 Token 数。\n\n> `max_tokens` 的设置不会影响大模型的生成过程，如果模型生成的 Token 数超过`max_tokens`，本次请求会返回截断后的内容。\n\n默认值和最大值都是模型的最大输出长度。关于各模型的最大输出长度，请参见[模型列表](https://help.aliyun.com/zh/model-studio/models#9f8890ce29g5u)。\n\nmax\\_tokens参数适用于需要限制字数（如生成摘要、关键词）、控制成本或减少响应时间的场景。\n\n> qwen-vl-ocr、qwen-vl-ocr-latest**、**qwen-vl-ocr-2025-04-13、qwen-vl-ocr-2025-08-28模型的`max_tokens`参数（最大输出长度）默认为 4096，如需提高该参数值（4097~8192范围），请发送邮件至 [modelstudio@service.aliyun.com](mailto:modelstudio@service.aliyun.com)进行申请，并提供以下信息：主账号ID、图像类型（如文档图、电商图、合同等）、模型名称、预计 QPS 和每日请求总数，以及模型输出长度超过4096的请求占比。\n\n> 对于 QwQ、QVQ 与开启思考模式的 Qwen3 模型，`max_tokens`会限制回复内容的长度，不限制深度思考内容的长度。\n\n**vl\\_high\\_resolution\\_images** `*boolean*` （可选）默认值为`false`\n\n是否提高输入图片的默认Token上限，适用于Qwen-VL、QVQ模型。\n\n-   False（默认值）：使用默认的Token处理图像\n    \n    -   `Qwen3-VL商业版及开源版`、`qwen-vl-max-0813`及以后、`qwen-vl-plus-0710`及以后更新的模型：默认Token上限为2560\n        \n    -   QVQ及其他Qwen-VL模型：默认Token上限为1280\n        \n-   True：输入图片的Token上限将提高为16384\n    \n\n**支持的模型**\n\n-   QVQ\n    \n-   Qwen3-VL\n    \n-   qwen-vl-max商业版：qwen-vl-max-0809及之后、qwen-vl-plus-0809及之后的模型\n    \n-   Qwen-VL开源：qwen2-vl、qwen2.5-vl、qwen3-vl\n    \n\n> 通过 Python SDK调用时，请将 **vl\\_high\\_resolution\\_images** 放入 **extra\\_body** 对象中，配置方式为：extra\\_body={\"vl\\_high\\_resolution\\_images\":xxx}。\n\n**n** `*integer*` （可选） 默认值为1\n\n生成响应的个数，取值范围是`1-4`。对于需要生成多个响应的场景（如创意写作、广告文案等），可以设置较大的 n 值。\n\n> 当前仅支持 qwen-plus、 [Qwen3（非思考模式）](https://help.aliyun.com/zh/model-studio/deep-thinking#be9890136awsc)、qwen-plus-character 模型，且在传入 tools 参数时固定为1。\n\n> 设置较大的 n 值不会增加输入 Token 消耗，会增加输出 Token 的消耗。\n\n**enable\\_thinking** `*boolean*` （可选）\n\n是否开启思考模式，适用于 Qwen3 、Qwen3-Omni-Flash、Qwen3-VL模型。\n\nQwen3 商业版模型默认值为 False，Qwen3 开源版模型默认值为 True。\n\n> 通过 Python SDK 调用时，请通过`extra_body`配置。配置方式为：`extra_body={\"enable_thinking\": xxx}`。\n\n**thinking\\_budget** `*integer*` （可选）\n\n思考过程的最大长度，只在`enable_thinking`为`true`时生效。适用于Qwen3-VL、Qwen3 的商业版与开源版模型。详情请参见[限制思考长度](https://help.aliyun.com/zh/model-studio/deep-thinking#e7c0002fe4meu)。\n\n> 默认值为模型最大思维链长度。\n\n> 通过 Python SDK 调用时，请通过`extra_body`配置。配置方式为：`extra_body={\"thinking_budget\": xxx}`。\n\n**seed** `*integer*` （可选）\n\n设置seed参数会使文本生成过程更具有确定性，通常用于使模型每次运行的结果一致。\n\n在每次模型调用时传入相同的seed值（由您指定），并保持其他参数不变，模型将尽可能返回相同的结果。\n\n取值范围：0到231−1。\n\n**seed默认值**\n\nqwen-vl-plus-2025-01-02、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08、qwen-vl-max-2025-04-02、qwen-vl-max-2024-12-30、qvq-72b-preview、qvq-max系列：3407；\n\nqwen-vl-max-2025-01-25、qwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen-vl-max-2024-02-01、qwen2-vl-72b-instruct、qwen2-vl-2b-instruct、qwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2025-05-07、qwen-vl-plus-2025-01-25、qwen-vl-plus-2024-08-09、qwen-vl-plus-2023-12-01：无默认值；\n\n其余模型均为1234。\n\n**logprobs** `*boolean*` （可选）默认值为 `false`\n\n是否返回输出 Token 的对数概率，可选值：\n\n-   true\n    \n    返回；\n    \n-   false\n    \n    不返回。\n    \n\n> 思考阶段生成的内容（`reasoning_content`）不会返回对数概率。\n\n> 支持 qwen-plus、qwen-turbo 系列的快照模型（不包含主线模型）与 Qwen3 开源模型。\n\n**top\\_logprobs** `*integer*` （可选）默认值为0\n\n指定在每一步生成时，返回模型最大概率的候选 Token 个数。\n\n取值范围：\\[0,5\\]\n\n仅当 `logprobs` 为 `true` 时生效。\n\n**stop** `*string 或 array*` （可选）\n\n使用stop参数后，当模型生成的文本即将包含指定的字符串或token\\_id时，将自动停止生成。\n\n您可以在stop参数中传入敏感词来控制模型的输出。\n\n> stop为array类型时，不可以将token\\_id和字符串同时作为元素输入，比如不可以指定stop为`[\"你好\",104307]`。\n\n**tools** `*array*` （可选）\n\n可供模型调用的工具数组，可以包含一个或多个工具对象。一次Function Calling流程模型会从中选择一个工具（开启[parallel\\_tool\\_calls](#df816f8ec85ry)可以选择多个工具）。\n\n> 目前不支持通义千问VL/Audio，也不建议用于数学和代码模型（Qwen3-Coder 模型除外）。\n\n**属性**\n\n**type** `*string*` **（必选）**\n\ntools的类型，当前仅支持function。\n\n**function** `*object*` **（必选）**\n\n**属性**\n\n**name** `*string*` **（必选）**\n\n工具函数的名称，必须是字母、数字，可以包含下划线和短划线，最大长度为64。\n\n**description** `*string*` **（必选）**\n\n工具函数的描述，供模型选择何时以及如何调用工具函数。\n\n**parameters** `*object*` **（必选）**\n\n工具的参数描述，需要是一个合法的JSON Schema。JSON Schema的描述可以见[链接](https://json-schema.org/understanding-json-schema)。如果parameters参数为空，表示function没有入参。\n\n**tool\\_choice** `*string 或 object*` （可选）默认值为 `\"auto\"`\n\n如果您希望对于某一类问题，大模型能够采取制定好的工具选择策略（如强制使用某个工具、强制不使用工具），可以通过修改`tool_choice`参数来强制指定工具调用的策略。可选值：\n\n-   `\"auto\"`\n    \n    表示由大模型进行工具策略的选择。\n    \n-   `\"none\"`\n    \n    如果您希望无论输入什么问题，Function Calling 都不会进行工具调用，可以设定`tool_choice`参数为`\"none\"`；\n    \n-   `{\"type\": \"function\", \"function\": {\"name\": \"the_function_to_call\"}}`\n    \n    如果您希望对于某一类问题，Function Calling 能够强制调用某个工具，可以设定`tool_choice`参数为`{\"type\": \"function\", \"function\": {\"name\": \"the_function_to_call\"}}`，其中`the_function_to_call`是您指定的工具函数名称。\n    \n\n**parallel\\_tool\\_calls** `*boolean*` （可选）默认值为 `false`\n\n是否开启并行工具调用。参数为`true`时开启，为`false`时不开启。并行工具调用详情请参见：[并行工具调用](https://help.aliyun.com/zh/model-studio/qwen-function-calling#cb6b5c484bt4x)。\n\n**translation\\_options** `*object*` （可选）\n\n当您使用[翻译模型](https://help.aliyun.com/zh/model-studio/machine-translation)时需要配置的翻译参数。\n\n**属性**\n\n**source\\_lang** `*string*` （必选）\n\n源语言的英文全称，详情请参见[支持的语言](https://help.aliyun.com/zh/model-studio/machine-translation#038d2865bbydc)。您可以将`source_lang`设置为`\"auto\"`，模型会自动判断输入文本属于哪种语言。\n\n**target\\_lang** `*string*` （必选）\n\n目标语言的英文全称，详情请参见[支持的语言](https://help.aliyun.com/zh/model-studio/machine-translation#038d2865bbydc)。\n\n**terms** `*arrays*` （可选）\n\n在使用[术语干预](https://help.aliyun.com/zh/model-studio/machine-translation#2bf54a5ab5voe)功能时需要设置的术语数组。\n\n**属性**\n\n**source** `*string*` （必选）\n\n源语言的术语。\n\n**target** `*string*` （必选）\n\n目标语言的术语。\n\n**tm\\_list** `*arrays*` （可选）\n\n在[翻译记忆](https://help.aliyun.com/zh/model-studio/machine-translation#17e15234e7gfp)功能时需要设置的翻译记忆数组。\n\n**属性**\n\n**source** `*string*` （必选）\n\n源语言的语句。\n\n**target** `*string*` （必选）\n\n目标语言的语句。\n\n**domains** `*string*` （可选）\n\n在使用[领域提示](https://help.aliyun.com/zh/model-studio/machine-translation#4af23a31db7lf)功能时需要设置的领域提示语句。\n\n> 领域提示语句暂时只支持英文。\n\n> 若您通过Python SDK调用，请通过extra\\_body配置。配置方式为：`extra_body={\"translation_options\": xxx}`。\n\n**enable\\_search** `*boolean*` （可选）\n\n模型在生成文本时是否使用互联网搜索结果进行参考。取值如下：\n\n-   true：启用互联网搜索，模型会将搜索结果作为文本生成过程中的参考信息，但模型会基于其内部逻辑判断是否使用互联网搜索结果。\n    \n    > 如果模型没有搜索互联网，建议优化Prompt，或设置`search_options`中的`forced_search`参数开启强制搜索。\n    \n-   false（默认）：关闭互联网搜索。\n    \n\n> 启用互联网搜索功能可能会增加 Token 的消耗。\n\n> 若您通过 Python SDK调用，请通过`extra_body`配置。配置方式为：`extra_body={\"enable_search\": True}`。\n\n**支持的模型**\n\n-   通义千问Max：qwen3-max、qwen3-max-2025-09-23、qwen3-max-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19及之后的快照版本\n    \n-   通义千问Plus：qwen-plus、qwen-plus-latest、qwen-plus-2025-07-14及之后的快照版本\n    \n-   通义千问Flash：qwen-flash、qwen-flash-2025-07-28及之后的快照版本\n    \n-   通义千问Turbo：qwen-turbo、qwen-turbo-latest、qwen-turbo-2025-07-15\n    \n-   QwQ：qwq-plus（仅支持流式输出）\n    \n-   Kimi：Moonshot-Kimi-K2-Instruct\n    \n\n**search\\_options** `*object*` （可选）\n\n联网搜索的策略。仅当`enable_search`为`true`时生效。详情参见[联网搜索](https://help.aliyun.com/zh/model-studio/web-search#cbddf5b28bug8)。\n\n**属性**\n\n**forced\\_search** *boolean*（可选）默认值为false\n\n是否强制开启搜索。参数值：\n\n-   true：强制开启；\n    \n-   false：不强制开启。\n    \n\n**search\\_strategy** `*string*`（可选）默认值为`turbo`\n\n搜索互联网信息的策略。参数值：\n\n-   `turbo`：默认策略，兼顾响应速度与搜索效果，推荐使用。\n    \n-   `max`：高性能模式，基于全栈顶配模型与多源搜索引擎，提供最优效果。\n    \n\n计费信息请参见[计费说明](https://help.aliyun.com/zh/model-studio/web-search#92ce83df3a599)。\n\n**enable\\_search\\_extension** `*boolean*`（可选）默认值为`false`\n\n是否开启特定领域增强。参数值：\n\n-   `true`\n    \n    开启。\n    \n-   `false`（默认值）\n    \n    不开启。\n    \n\n> 若您通过 Python SDK调用，请通过`extra_body`配置。配置方式为：`extra_body={\"search_options\": xxx}`。\n\n**X-DashScope-DataInspection** `*string*` （可选）\n\n在通义千问 API 的内容安全能力基础上，是否进一步识别输入输出内容的违规信息。取值如下：\n\n-   `'{\"input\":\"cip\",\"output\":\"cip\"}'`：进一步识别；\n    \n-   不设置该参数：不进一步识别。\n    \n\n通过 HTTP 调用时请放入请求头：`-H \"X-DashScope-DataInspection: {\\\"input\\\": \\\"cip\\\", \\\"output\\\": \\\"cip\\\"}\"`；\n\n通过 Python SDK 调用时请通过`extra_headers`配置：`extra_headers={'X-DashScope-DataInspection': '{\"input\":\"cip\",\"output\":\"cip\"}'}`。\n\n详细使用方法请参见[内容审核](https://help.aliyun.com/zh/model-studio/content-security)。\n\n> 不支持通过 Node.js SDK设置。\n\n> 不适用于 Qwen-VL 系列模型。\n\n### chat响应对象（非流式输出）\n\n```\n{\n    \"choices\": [\n        {\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"我是阿里云开发的一款超大规模语言模型，我叫通义千问。\"\n            },\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"logprobs\": null\n        }\n    ],\n    \"object\": \"chat.completion\",\n    \"usage\": {\n        \"prompt_tokens\": 3019,\n        \"completion_tokens\": 104,\n        \"total_tokens\": 3123,\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 2048\n        }\n    },\n    \"created\": 1735120033,\n    \"system_fingerprint\": null,\n    \"model\": \"qwen-plus\",\n    \"id\": \"chatcmpl-6ada9ed2-7f33-9de2-8bb0-78bd4035025a\"\n}\n```\n\n**id** `*string*`\n\n本次调用的唯一标识符。\n\n**choices** `*array*`\n\n模型生成内容的数组，可以包含一个或多个choices对象。\n\n**属性**\n\n**finish\\_reason** `*string*`\n\n有三种情况：\n\n-   因触发输入参数中的stop条件，或自然停止输出时为`stop`；\n    \n-   因生成长度过长而结束为`length`；\n    \n-   因需要调用工具而结束为`tool_calls`。\n    \n\n**index** `*integer*`\n\n当前响应在`choices`数组中的序列编号。\n\n**logprobs** `*object*`\n\n当前 choices 对象的概率信息。\n\n**属性**\n\n**content** `*array*`\n\n带有对数概率信息的 Token 数组。\n\n**属性**\n\n**token** `*string*`\n\n当前 Token。\n\n**bytes** `*array*`\n\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\n\n**logprob** `*float*`\n\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\n\n**top\\_logprobs** `*array*`\n\n当前 Token 位置最可能的若干个 Token 及其对数概率，元素个数与入参的`top_logprobs`保持一致。\n\n**属性**\n\n**token** `*string*`\n\n当前 Token。\n\n**bytes** `*array*`\n\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\n\n**logprob** `*float*`\n\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\n\n**message** `*object*`\n\n本次调用模型输出的消息。\n\n**属性**\n\n**content** `*string*`\n\n本次调用模型生成的文本。\n\n**reasoning\\_content** `*string*`\n\n模型的思维链内容。\n\n**refusal** `*string*`\n\n该参数当前固定为`null`。\n\n**role** `*string*`\n\n消息的角色，固定为`assistant`。\n\n**audio** `*object*`\n\n该参数当前固定为`null`。\n\n**function\\_call**（即将废弃）`*object*`\n\n该值默认为`null`，请参考`tool_calls`参数。\n\n**tool\\_calls** `*array*`\n\n在发起 Function Calling后，模型回复的要调用的工具以及调用工具所需的参数。可以包含一个或多个工具响应对象。\n\n**属性**\n\n**id** `*string*`\n\n本次工具响应的ID。\n\n**type** `*string*`\n\n工具的类型，当前只支持`function`。\n\n**function** `*object*`\n\n需要被调用的函数。\n\n**属性**\n\n**name** `*string*`\n\n需要被调用的函数名。\n\n**arguments** `*string*`\n\n需要输入到工具中的参数，为JSON字符串。\n\n> 由于大模型响应有一定随机性，输出的JSON字符串并不总满足于您的函数，建议您在将参数输入函数前进行参数的有效性校验。\n\n**index** `*integer*`\n\n工具信息在`tool_calls`列表中的索引。\n\n**created** `*integer*`\n\n本次chat请求被创建时的时间戳。\n\n**model** `*string*`\n\n本次chat请求使用的模型名称。\n\n**object** `*string*`\n\n始终为`chat.completion`。\n\n**service\\_tier** `*string*`\n\n该参数当前固定为`null`。\n\n**system\\_fingerprint** `*string*`\n\n该参数当前固定为`null`。\n\n**usage** `*object*`\n\n本次chat请求使用的 Token 信息。\n\n**属性**\n\n**completion\\_tokens** `*integer*`\n\n模型生成回复转换为 Token 后的长度。\n\n**prompt\\_tokens** `*integer*`\n\n用户的输入转换成 Token 后的长度。\n\n**total\\_tokens** `*integer*`\n\n`prompt_tokens`与`completion_tokens`的总和。\n\n**completion\\_tokens\\_details** `*object*`\n\n使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)时输出Token的细粒度分类。\n\n**属性**\n\n**audio\\_tokens** `*integer*`\n\n该参数当前固定为`null`。\n\n**reasoning\\_tokens** `*integer*`\n\n该参数当前固定为`null`。\n\n**text\\_tokens** `*integer*`\n\n[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)输出的文本的Token数。\n\n**prompt\\_tokens\\_details** `*object*`\n\n输入 Token 的细粒度分类。\n\n**属性**\n\n**audio\\_tokens** `*integer*`\n\n该参数当前固定为`null`。\n\n**cached\\_tokens** `*integer*`\n\n命中 Cache 的 Token 数。Context Cache 详情请参见[上下文缓存](https://help.aliyun.com/zh/model-studio/context-cache)。\n\n**text\\_tokens** `*integer*`\n\n[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)输入的文本转换为 Token 后的长度。\n\n**image\\_tokens** `*integer*`\n\n[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)输入的图像转换为Token的长度。\n\n**video\\_tokens** `*integer*`\n\n[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)输入的视频文件或者图像列表转换为Token后的长度。\n\n**cache\\_creation** `*object*`\n\n[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)创建信息。\n\n**属性**\n\n**ephemeral\\_5m\\_input\\_tokens** `*integer*`\n\n用于创建5分钟有效期显式缓存的 Token 长度。\n\n**cache\\_creation\\_input\\_tokens** `*integer*`\n\n用于创建显式缓存的 Token 长度。\n\n**cache\\_type** `*string*`\n\n使用[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)时，参数值为`ephemeral`，否则该参数不存在。\n\n### chat响应chunk对象（流式输出）\n\n```\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":\"assistant\",\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"我是\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"来自\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"阿里\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"云的超大规模\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"语言模型，我\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"叫通义千\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"问。\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":17,\"prompt_tokens\":22,\"total_tokens\":39,\"completion_tokens_details\":null,\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}\n```\n\n**id** `*string*`\n\n本次调用的唯一标识符。每个chunk对象有相同的 id。\n\n**choices** `*array*`\n\n模型生成内容的数组，可包含一个或多个choices对象。如果设置`include_usage`参数为`true`，则最后一个chunk为空。\n\n**属性**\n\n**delta** `*object*`\n\nchat请求的增量对象。\n\n**属性**\n\n**content** `*string*`\n\nchunk的消息内容。\n\n**reasoning\\_content** `*string*`\n\n模型的思维链内容。\n\n**function\\_call** `*object*`\n\n该值默认为`null`，请参考`tool_calls`参数。\n\n**audio** `*object*`\n\n使用 [Qwen-Omni](https://help.aliyun.com/zh/model-studio/qwen-omni) 模型时生成的回复。\n\n**属性**\n\n**transcript** `*string*`\n\n流式输出的文本内容。\n\n**data** `*string*`\n\n流式输出的 Base64 音频编码数据。\n\n**expires\\_at** `*integer*`\n\n创建请求时的时间戳。\n\n**refusal** `*object*`\n\n该参数当前固定为`null`。\n\n**role** `*string*`\n\n增量消息对象的角色，只在第一个chunk中有值。\n\n**tool\\_calls** `*array*`\n\n模型回复的要调用的工具以及调用工具所需的参数。可以包含一个或多个工具响应对象。\n\n**属性**\n\n**index** `*integer*`\n\n工具信息在`tool_calls`列表中的索引。\n\n**id** `*string*`\n\n本次工具响应的ID。\n\n**function** `*object*`\n\n需要被调用的函数。\n\n**属性**\n\n**arguments** `*string*`\n\n需要输入到工具中的参数，所有chunk的arguments拼接后为完整的JSON字符串。\n\n> 由于大模型响应有一定随机性，输出的JSON字符串并不总满足于您的函数，建议您在将参数输入函数前进行参数的有效性校验。\n\n**name** `*string*`\n\n函数名称，只在第一个chunk中有值。\n\n**type** `*string*`\n\n工具的类型，当前只支持`function`。\n\n**finish\\_reason** `*string*`\n\n有四种情况：\n\n-   因触发输入参数中的stop条件，或自然停止输出时为`stop`；\n    \n-   当生成未结束时为`null`；\n    \n-   因生成长度过长而结束为`length`；\n    \n-   因需要调用工具而结束为`tool_calls`。\n    \n\n**index** `*integer*`\n\n当前响应在`choices`列表中的序列编号。当输入参数 [n](#5dc40bb1b1mc0) 大于1时，您需要根据 `index` 参数来进行不同响应对应的完整内容的拼接。\n\n**logprobs** `*object*`\n\n当前 choices 对象的概率信息。\n\n**属性**\n\n**content** `*array*`\n\n带有对数概率信息的 Token 数组。\n\n**属性**\n\n**token** `*string*`\n\n当前 Token。\n\n**bytes** `*array*`\n\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\n\n**logprob** `*float*`\n\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\n\n**top\\_logprobs** `*array*`\n\n当前 Token 位置最可能的若干个 Token 及其对数概率，元素个数与入参的`top_logprobs`保持一致。\n\n**属性**\n\n**token** `*string*`\n\n当前 Token。\n\n**bytes** `*array*`\n\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\n\n**logprob** `*float*`\n\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\n\n**created** `*integer*`\n\n本次chat请求被创建时的时间戳。每个chunk对象有相同的时间戳。\n\n**model** `*string*`\n\n本次chat请求使用的模型名称。\n\n**object** `*string*`\n\n始终为`chat.completion.chunk`。\n\n**service\\_tier** `*string*`\n\n该参数当前固定为`null`。\n\n**system\\_fingerprint**`*string*`\n\n该参数当前固定为`null`。\n\n**usage** `*object*`\n\n本次chat请求使用的Token信息。只在`include_usage`为`true`时，在最后一个chunk显示。\n\n**属性**\n\n**completion\\_tokens** `*integer*`\n\n模型生成回复转换为 Token 后的长度。\n\n**prompt\\_tokens** `*integer*`\n\n用户的输入转换成 Token 后的长度。\n\n**total\\_tokens** `*integer*`\n\n`prompt_tokens`与`completion_tokens`的总和。\n\n**completion\\_tokens\\_details** `*object*`\n\n输出转换为 Token 后的详细信息。\n\n**属性**\n\n**audio\\_tokens** `*integer*`\n\n[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)输出的音频转换为 Token 后的长度。\n\n**reasoning\\_tokens** `*integer*`\n\nQwen3 模型思考过程转换为 Token 后的长度。\n\n**text\\_tokens** `*integer*`\n\n[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)、[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)输出的文本转换为 Token 后的长度。\n\n**prompt\\_tokens\\_details** `*object*`\n\n输入数据的 Token 细粒度分类。\n\n**属性**\n\n**audio\\_tokens** `*integer*`\n\n使用[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)时，输入的音频转换为 Token 后的长度。\n\n> 视频文件中的音频转换为 Token 后的长度会在该参数中体现。\n\n**text\\_tokens** `*integer*`\n\n使用[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)时，输入的文本转换为 Token 后的长度。\n\n**video\\_tokens** `*integer*`\n\n使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)、[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)时，输入的视频（图片列表形式或视频文件）转换为 Token 后的长度。如果[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)输入的是视频文件，则`video_tokens` 不包含音频的 Token，音频的 Token 在`audio_tokens`中体现。\n\n**image\\_tokens** `*integer*`\n\n使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)、[Qwen-Omni 模型](https://help.aliyun.com/zh/model-studio/qwen-omni)时，输入的图片转换为 Token 后的长度。\n\n**cached\\_tokens** `*integer*`\n\n命中 Cache 的 Token 数。Context Cache 详情请参见[上下文缓存](https://help.aliyun.com/zh/model-studio/context-cache)。\n\n**cache\\_creation** `*object*`\n\n[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)创建信息。\n\n**属性**\n\n**ephemeral\\_5m\\_input\\_tokens** `*integer*`\n\n用于创建5分钟有效期显式缓存的 Token 长度。\n\n**cache\\_creation\\_input\\_tokens** `*integer*`\n\n用于创建显式缓存的 Token 长度。\n\n**cache\\_type** `*string*`\n\n使用[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)时，参数值为`ephemeral`，否则该参数不存在\n\n## DashScope\n\n## 北京地域\n\n**通过HTTP调用时需配置的endpoint：**\n\n使用通义千问大语言模型：`POST https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation`\n\n使用通义千问VL或通义千问Audio模型：`POST https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation`\n\n## 新加坡地域\n\n**通过HTTP调用时需配置的endpoint：**\n\n-   使用通义千问大语言模型：`POST https://dashscope-intl.aliyuncs.com/api/v1/services/aigc/text-generation/generation`\n    \n-   使用通义千问VL/OCR模型：`POST https://dashscope-intl.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation`\n    \n\n**通过SDK调用时需配置的base\\_url：**\n\n## **Python代码**\n\n```\ndashscope.base_http_api_url = 'https://dashscope-intl.aliyuncs.com/api/v1'\n```\n\n## **Java代码**\n\n-   **方式一：**\n    \n    ```\n    import com.alibaba.dashscope.protocol.Protocol;\n    Generation gen = new Generation(Protocol.HTTP.getValue(), \"https://dashscope-intl.aliyuncs.com/api/v1\");\n    ```\n    \n-   **方式二：**\n    \n    ```\n    import com.alibaba.dashscope.utils.Constants;\n    Constants.baseHttpApiUrl=\"https://dashscope-intl.aliyuncs.com/api/v1\";\n    ```\n    \n\n## 金融云\n\n**通过HTTP调用时需配置的endpoint：**\n\n使用通义千问大语言模型：`POST https://dashscope-finance.aliyuncs.com/api/v1/services/aigc/text-generation/generation`\n\n**通过SDK调用时需配置的base\\_url：**`https://dashscope-finance.aliyuncs.com/api/v1`\n\n## **Python代码**\n\n在导入模块之后，添加以下代码：\n\n```\ndashscope.base_http_api_url = 'https://dashscope-finance.aliyuncs.com/api/v1'\n```\n\n## **Java**代码\n\n```\nGeneration gen = new Generation(\"http\", \"https://dashscope-finance.aliyuncs.com/api/v1\");\n```\n\n> 您需要已[获取API Key](https://help.aliyun.com/zh/model-studio/get-api-key)并[配置API Key到环境变量](https://help.aliyun.com/zh/model-studio/configure-api-key-through-environment-variables)。如果通过DashScope SDK进行调用，还需要[安装DashScope SDK](https://help.aliyun.com/zh/model-studio/install-sdk#f3e80b21069aa)。\n\n### 请求体\n\n## 文本输入\n\n> 此处以单轮对话作为示例，您也可以进行[多轮对话](https://help.aliyun.com/zh/model-studio/multi-round-conversation)。\n\n## Python\n\n```\nimport os\nimport dashscope\n\nmessages = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'},\n    {'role': 'user', 'content': '你是谁？'}\n]\nresponse = dashscope.Generation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv('DASHSCOPE_API_KEY'),\n    model=\"qwen-plus\", # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages,\n    result_format='message'\n    )\nprint(response)\n```\n\n## Java\n\n```\n// 建议dashscope SDK的版本 >= 2.12.0\nimport java.util.Arrays;\nimport java.lang.System;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.utils.JsonUtils;\n\npublic class Main {\n    public static GenerationResult callWithMessage() throws ApiException, NoApiKeyException, InputRequiredException {\n        Generation gen = new Generation();\n        Message systemMsg = Message.builder()\n                .role(Role.SYSTEM.getValue())\n                .content(\"You are a helpful assistant.\")\n                .build();\n        Message userMsg = Message.builder()\n                .role(Role.USER.getValue())\n                .content(\"你是谁？\")\n                .build();\n        GenerationParam param = GenerationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n                .model(\"qwen-plus\")\n                .messages(Arrays.asList(systemMsg, userMsg))\n                .resultFormat(GenerationParam.ResultFormat.MESSAGE)\n                .build();\n        return gen.call(param);\n    }\n    public static void main(String[] args) {\n        try {\n            GenerationResult result = callWithMessage();\n            System.out.println(JsonUtils.toJson(result));\n        } catch (ApiException | NoApiKeyException | InputRequiredException e) {\n            // 使用日志框架记录异常信息\n            System.err.println(\"An error occurred while calling the generation service: \" + e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n## PHP（HTTP）\n\n```\n<?php\n\n$url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\";\n$apiKey = getenv('DASHSCOPE_API_KEY');\n\n$data = [\n    // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    \"model\" => \"qwen-plus\",\n    \"input\" => [\n        \"messages\" => [\n            [\n                \"role\" => \"system\",\n                \"content\" => \"You are a helpful assistant.\"\n            ],\n            [\n                \"role\" => \"user\",\n                \"content\" => \"你是谁？\"\n            ]\n        ]\n    ],\n    \"parameters\" => [\n        \"result_format\" => \"message\"\n    ]\n];\n\n$jsonData = json_encode($data);\n\n$ch = curl_init($url);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $jsonData);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, [\n    \"Authorization: Bearer $apiKey\",\n    \"Content-Type: application/json\"\n]);\n\n$response = curl_exec($ch);\n$httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\n\nif ($httpCode == 200) {\n    echo \"Response: \" . $response;\n} else {\n    echo \"Error: \" . $httpCode . \" - \" . $response;\n}\n\ncurl_close($ch);\n?>\n```\n\n## Node.js（HTTP）\n\nDashScope 未提供 Node.js 环境的 SDK。如需通过 OpenAI Node.js SDK调用，请参考本文的[OpenAI](#4ec3e641c294d)章节。\n\n```\nimport fetch from 'node-fetch';\n\nconst apiKey = process.env.DASHSCOPE_API_KEY;\n\nconst data = {\n    model: \"qwen-plus\", // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    input: {\n        messages: [\n            {\n                role: \"system\",\n                content: \"You are a helpful assistant.\"\n            },\n            {\n                role: \"user\",\n                content: \"你是谁？\"\n            }\n        ]\n    },\n    parameters: {\n        result_format: \"message\"\n    }\n};\n\nfetch('https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation', {\n    method: 'POST',\n    headers: {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify(data)\n})\n.then(response => response.json())\n.then(data => {\n    console.log(JSON.stringify(data));\n})\n.catch(error => {\n    console.error('Error:', error);\n});\n```\n\n## C#（HTTP）\n\n```\nusing System.Net.Http.Headers;\nusing System.Text;\n\nclass Program\n{\n    private static readonly HttpClient httpClient = new HttpClient();\n\n    static async Task Main(string[] args)\n    {\n        // 若没有配置环境变量，请用百炼API Key将下行替换为：string? apiKey = \"sk-xxx\";\n        string? apiKey = Environment.GetEnvironmentVariable(\"DASHSCOPE_API_KEY\");\n\n        if (string.IsNullOrEmpty(apiKey))\n        {\n            Console.WriteLine(\"API Key 未设置。请确保环境变量 'DASHSCOPE_API_KEY' 已设置。\");\n            return;\n        }\n\n        // 设置请求 URL 和内容\n        string url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\";\n        // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        string jsonContent = @\"{\n            \"\"model\"\": \"\"qwen-plus\"\", \n            \"\"input\"\": {\n                \"\"messages\"\": [\n                    {\n                        \"\"role\"\": \"\"system\"\",\n                        \"\"content\"\": \"\"You are a helpful assistant.\"\"\n                    },\n                    {\n                        \"\"role\"\": \"\"user\"\",\n                        \"\"content\"\": \"\"你是谁？\"\"\n                    }\n                ]\n            },\n            \"\"parameters\"\": {\n                \"\"result_format\"\": \"\"message\"\"\n            }\n        }\";\n\n        // 发送请求并获取响应\n        string result = await SendPostRequestAsync(url, jsonContent, apiKey);\n\n        // 输出结果\n        Console.WriteLine(result);\n    }\n\n    private static async Task<string> SendPostRequestAsync(string url, string jsonContent, string apiKey)\n    {\n        using (var content = new StringContent(jsonContent, Encoding.UTF8, \"application/json\"))\n        {\n            // 设置请求头\n            httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\"));\n\n            // 发送请求并获取响应\n            HttpResponseMessage response = await httpClient.PostAsync(url, content);\n\n            // 处理响应\n            if (response.IsSuccessStatusCode)\n            {\n                return await response.Content.ReadAsStringAsync();\n            }\n            else\n            {\n                return $\"请求失败: {response.StatusCode}\";\n            }\n        }\n    }\n}\n```\n\n## Go（HTTP）\n\nDashScope 未提供 Go 的 SDK。如需通过 OpenAI Go SDK调用，请参考本文的[OpenAI-Go](#0d97c51c65umq)章节。\n\n```\npackage main\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"net/http\"\n\t\"os\"\n)\n\ntype Message struct {\n\tRole    string `json:\"role\"`\n\tContent string `json:\"content\"`\n}\n\ntype Input struct {\n\tMessages []Message `json:\"messages\"`\n}\n\ntype Parameters struct {\n\tResultFormat string `json:\"result_format\"`\n}\n\ntype RequestBody struct {\n\tModel      string     `json:\"model\"`\n\tInput      Input      `json:\"input\"`\n\tParameters Parameters `json:\"parameters\"`\n}\n\nfunc main() {\n\t// 创建 HTTP 客户端\n\tclient := &http.Client{}\n\n\t// 构建请求体\n\trequestBody := RequestBody{\n\t\t// 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n\t\tModel: \"qwen-plus\",\n\t\tInput: Input{\n\t\t\tMessages: []Message{\n\t\t\t\t{\n\t\t\t\t\tRole:    \"system\",\n\t\t\t\t\tContent: \"You are a helpful assistant.\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tRole:    \"user\",\n\t\t\t\t\tContent: \"你是谁？\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tParameters: Parameters{\n\t\t\tResultFormat: \"message\",\n\t\t},\n\t}\n\n\tjsonData, err := json.Marshal(requestBody)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// 创建 POST 请求\n\treq, err := http.NewRequest(\"POST\", \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\", bytes.NewBuffer(jsonData))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// 设置请求头\n\t// 若没有配置环境变量，请用百炼API Key将下行替换为：apiKey := \"sk-xxx\"\n\tapiKey := os.Getenv(\"DASHSCOPE_API_KEY\")\n\treq.Header.Set(\"Authorization\", \"Bearer \"+apiKey)\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\n\t// 发送请求\n\tresp, err := client.Do(req)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer resp.Body.Close()\n\n\t// 读取响应体\n\tbodyText, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// 打印响应内容\n\tfmt.Printf(\"%s\\n\", bodyText)\n}\n```\n\n## curl\n\n```\ncurl --location \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model\": \"qwen-plus\",\n    \"input\":{\n        \"messages\":[      \n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"你是谁？\"\n            }\n        ]\n    },\n    \"parameters\": {\n        \"result_format\": \"message\"\n    }\n}'\n```\n\n## 流式输出\n\n> 更多用法请参见[流式输出](https://help.aliyun.com/zh/model-studio/stream)。\n\n## Python\n\n```\nimport os\nimport dashscope\n\nmessages = [\n    {'role':'system','content':'you are a helpful assistant'},\n    {'role': 'user','content': '你是谁？'}\n]\nresponses = dashscope.Generation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv('DASHSCOPE_API_KEY'),\n    model=\"qwen-plus\", # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages,\n    result_format='message',\n    stream=True,\n    incremental_output=True\n    )\nfor response in responses:\n    print(response)  \n```\n\n## Java\n\n```\nimport java.util.Arrays;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.utils.JsonUtils;\nimport io.reactivex.Flowable;\nimport java.lang.System;\n\npublic class Main {\n    private static final Logger logger = LoggerFactory.getLogger(Main.class);\n    private static void handleGenerationResult(GenerationResult message) {\n        System.out.println(JsonUtils.toJson(message));\n    }\n    public static void streamCallWithMessage(Generation gen, Message userMsg)\n            throws NoApiKeyException, ApiException, InputRequiredException {\n        GenerationParam param = buildGenerationParam(userMsg);\n        Flowable<GenerationResult> result = gen.streamCall(param);\n        result.blockingForEach(message -> handleGenerationResult(message));\n    }\n    private static GenerationParam buildGenerationParam(Message userMsg) {\n        return GenerationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n                .model(\"qwen-plus\")\n                .messages(Arrays.asList(userMsg))\n                .resultFormat(GenerationParam.ResultFormat.MESSAGE)\n                .incrementalOutput(true)\n                .build();\n    }\n    public static void main(String[] args) {\n        try {\n            Generation gen = new Generation();\n            Message userMsg = Message.builder().role(Role.USER.getValue()).content(\"你是谁？\").build();\n            streamCallWithMessage(gen, userMsg);\n        } catch (ApiException | NoApiKeyException | InputRequiredException  e) {\n            logger.error(\"An exception occurred: {}\", e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n## curl\n\n```\ncurl --location \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--header \"X-DashScope-SSE: enable\" \\\n--data '{\n    \"model\": \"qwen-plus\",\n    \"input\":{\n        \"messages\":[      \n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"你是谁？\"\n            }\n        ]\n    },\n    \"parameters\": {\n        \"result_format\": \"message\",\n        \"incremental_output\":true\n    }\n}'\n```\n\n## 图像输入\n\n> 关于大模型分析图像的更多用法，请参见[视觉理解](https://help.aliyun.com/zh/model-studio/vision)。\n\n## Python\n\n```\nimport os\nimport dashscope\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"},\n            {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/tiger.png\"},\n            {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/rabbit.png\"},\n            {\"text\": \"这些是什么?\"}\n        ]\n    }\n]\nresponse = dashscope.MultiModalConversation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv('DASHSCOPE_API_KEY'),\n    model='qwen-vl-max', # 此处以qwen-vl-max为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages\n    )\nprint(response)\n```\n\n## Java\n\n```\n// Copyright (c) Alibaba, Inc. and its affiliates.\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\n    public static void simpleMultiModalConversationCall()\n            throws ApiException, NoApiKeyException, UploadFileException {\n        MultiModalConversation conv = new MultiModalConversation();\n        MultiModalMessage userMessage = MultiModalMessage.builder().role(Role.USER.getValue())\n                .content(Arrays.asList(\n                        Collections.singletonMap(\"image\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"),\n                        Collections.singletonMap(\"image\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/tiger.png\"),\n                        Collections.singletonMap(\"image\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/rabbit.png\"),\n                        Collections.singletonMap(\"text\", \"这些是什么?\"))).build();\n        MultiModalConversationParam param = MultiModalConversationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                // 此处以qwen-vl-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n                .model(\"qwen-vl-plus\")\n                .message(userMessage)\n                .build();\n        MultiModalConversationResult result = conv.call(param);\n        System.out.println(JsonUtils.toJson(result));\n    }\n\n    public static void main(String[] args) {\n        try {\n            simpleMultiModalConversationCall();\n        } catch (ApiException | NoApiKeyException | UploadFileException e) {\n            System.out.println(e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n## curl\n\n```\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"model\": \"qwen-vl-plus\",\n    \"input\":{\n        \"messages\":[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"},\n                    {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/tiger.png\"},\n                    {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/rabbit.png\"},\n                    {\"text\": \"这些是什么?\"}\n                ]\n            }\n        ]\n    }\n}'\n```\n\n## 视频输入\n\n> 以下为传入视频帧的示例代码，关于更多用法（如传入视频文件），请参见[视觉理解](https://help.aliyun.com/zh/model-studio/vision#80dbf6ca8fh6s)。\n\n## Python\n\n```\nfrom http import HTTPStatus\nimport os\n# dashscope版本需要不低于1.20.10\nimport dashscope\n\nmessages = [{\"role\": \"user\",\n             \"content\": [\n                 {\"video\":[\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n                           \"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n                           \"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n                           \"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"]},\n                 {\"text\": \"描述这个视频的具体过程\"}]}]\nresponse = dashscope.MultiModalConversation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    model='qwen-vl-max-latest',  # 此处以qwen-vl-max-latest为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages\n)\nif response.status_code == HTTPStatus.OK:\n    print(response)\nelse:\n    print(response.code)\n    print(response.message)\n```\n\n## Java\n\n```\n// DashScope SDK版本需要不低于2.16.7\nimport java.util.Arrays;\nimport java.util.Collections;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\n    // 此处以qwen-vl-max-latest为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    private static final String MODEL_NAME = \"qwen-vl-max-latest\";\n    public static void videoImageListSample() throws ApiException, NoApiKeyException, UploadFileException {\n        MultiModalConversation conv = new MultiModalConversation();\n        MultiModalMessage systemMessage = MultiModalMessage.builder()\n                .role(Role.SYSTEM.getValue())\n                .content(Arrays.asList(Collections.singletonMap(\"text\", \"You are a helpful assistant.\")))\n                .build();\n        MultiModalMessage userMessage = MultiModalMessage.builder()\n                .role(Role.USER.getValue())\n                .content(Arrays.asList(Collections.singletonMap(\"video\", Arrays.asList(\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n                                \"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n                                \"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n                                \"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\")),\n                        Collections.singletonMap(\"text\", \"描述这个视频的具体过程\")))\n                .build();\n        MultiModalConversationParam param = MultiModalConversationParam.builder()\n                .model(MODEL_NAME).message(systemMessage)\n                .message(userMessage).build();\n        MultiModalConversationResult result = conv.call(param);\n        System.out.print(JsonUtils.toJson(result));\n    }\n    public static void main(String[] args) {\n        try {\n            videoImageListSample();\n        } catch (ApiException | NoApiKeyException | UploadFileException e) {\n            System.out.println(e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n  \"model\": \"qwen-vl-max-latest\",\n  \"input\": {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"video\": [\n              \"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n              \"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n              \"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n              \"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"\n            ]\n          },\n          {\n            \"text\": \"描述这个视频的具体过程\"\n          }\n        ]\n      }\n    ]\n  }\n}'\n```\n\n## 音频输入\n\n## 音频理解\n\n> 关于大模型分析音频的更多用法，请参见[音频理解-Qwen-Audio](https://help.aliyun.com/zh/model-studio/audio-language-model)。\n\n## Python\n\n```\nimport os\nimport dashscope\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"},\n            {\"text\": \"这段音频在说什么?\"}\n        ]\n    }\n]\nresponse = dashscope.MultiModalConversation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv('DASHSCOPE_API_KEY'),\n    model='qwen2-audio-instruct', # 此处以qwen2-audio-instruct为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages\n    )\nprint(response)\n```\n\n## Java\n\n```\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.lang.System;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\n    public static void simpleMultiModalConversationCall()\n            throws ApiException, NoApiKeyException, UploadFileException {\n        MultiModalConversation conv = new MultiModalConversation();\n        MultiModalMessage userMessage = MultiModalMessage.builder().role(Role.USER.getValue())\n                .content(Arrays.asList(Collections.singletonMap(\"audio\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"),\n                        Collections.singletonMap(\"text\", \"这段音频在说什么?\"))).build();\n        MultiModalConversationParam param = MultiModalConversationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                // 此处以qwen2-audio-instruct为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n                .model(\"qwen2-audio-instruct\")\n                .message(userMessage)\n                .build();\n        MultiModalConversationResult result = conv.call(param);\n        System.out.println(JsonUtils.toJson(result));\n    }\n\n    public static void main(String[] args) {\n        try {\n            simpleMultiModalConversationCall();\n        } catch (ApiException | NoApiKeyException | UploadFileException e) {\n            System.out.println(e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n## curl\n\n```\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"model\": \"qwen2-audio-instruct\",\n    \"input\":{\n        \"messages\":[\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\"text\": \"You are a helpful assistant.\"}\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"},\n                    {\"text\": \"这段音频在说什么?\"}\n                ]\n            }\n        ]\n    }\n}'\n```\n\n## 语音识别\n\n> 关于语音识别的更多用法，请参见[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)。\n\n### Python\n\n```\nimport os\nimport dashscope\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            # 此处用于配置定制化识别的Context\n            {\"text\": \"\"},\n        ]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"},\n        ]\n    }\n]\nresponse = dashscope.MultiModalConversation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key = \"sk-xxx\"\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    model=\"qwen3-asr-flash\",\n    messages=messages,\n    result_format=\"message\",\n    asr_options={\n        # \"language\": \"zh\", # 可选，若已知音频的语种，可通过该参数指定待识别语种，以提升识别准确率\n        \"enable_lid\":True,\n        \"enable_itn\":False\n    }\n)\nprint(response)\n```\n\n完整结果以JSON格式输出到控制台。完整结果包含状态码、唯一的请求ID、识别后的内容以及本次调用的token信息。\n\n```\n{\n    \"output\": {\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"message\": {\n                    \"annotations\": [\n                        {\n                            \"language\": \"zh\",\n                            \"type\": \"audio_info\"\n                        }\n                    ],\n                    \"content\": [\n                        {\n                            \"text\": \"欢迎使用阿里云。\"\n                        }\n                    ],\n                    \"role\": \"assistant\"\n                }\n            }\n        ]\n    },\n    \"usage\": {\n        \"input_tokens_details\": {\n            \"text_tokens\": 0\n        },\n        \"output_tokens_details\": {\n            \"text_tokens\": 6\n        },\n        \"seconds\": 1\n    },\n    \"request_id\": \"568e2bf0-d6f2-97f8-9f15-a57b11dc6977\"\n}\n```\n\n### Java\n\n```\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\n\npublic class Main {\n    public static void simpleMultiModalConversationCall()\n            throws ApiException, NoApiKeyException, UploadFileException {\n        MultiModalConversation conv = new MultiModalConversation();\n        MultiModalMessage userMessage = MultiModalMessage.builder()\n                .role(Role.USER.getValue())\n                .content(Arrays.asList(\n                        Collections.singletonMap(\"audio\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\")))\n                .build();\n\n        MultiModalMessage sysMessage = MultiModalMessage.builder().role(Role.SYSTEM.getValue())\n                // 此处用于配置定制化识别的Context\n                .content(Arrays.asList(Collections.singletonMap(\"text\", \"\")))\n                .build();\n\n        Map<String, Object> asrOptions = new HashMap<>();\n        asrOptions.put(\"enable_lid\", true);\n        asrOptions.put(\"enable_itn\", false);\n        // asrOptions.put(\"language\", \"zh\"); // 可选，若已知音频的语种，可通过该参数指定待识别语种，以提升识别准确率\n        MultiModalConversationParam param = MultiModalConversationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                .model(\"qwen3-asr-flash\")\n                .message(userMessage)\n                .message(sysMessage)\n                .parameter(\"asr_options\", asrOptions)\n                .build();\n        MultiModalConversationResult result = conv.call(param);\n        System.out.println(JsonUtils.toJson(result));\n    }\n    public static void main(String[] args) {\n        try {\n            simpleMultiModalConversationCall();\n        } catch (ApiException | NoApiKeyException | UploadFileException e) {\n            System.out.println(e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n完整结果以JSON格式输出到控制台。完整结果包含状态码、唯一的请求ID、识别后的内容以及本次调用的token信息。\n\n```\n{\n    \"output\": {\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"message\": {\n                    \"annotations\": [\n                        {\n                            \"language\": \"zh\",\n                            \"type\": \"audio_info\"\n                        }\n                    ],\n                    \"content\": [\n                        {\n                            \"text\": \"欢迎使用阿里云。\"\n                        }\n                    ],\n                    \"role\": \"assistant\"\n                }\n            }\n        ]\n    },\n    \"usage\": {\n        \"input_tokens_details\": {\n            \"text_tokens\": 0\n        },\n        \"output_tokens_details\": {\n            \"text_tokens\": 6\n        },\n        \"seconds\": 1\n    },\n    \"request_id\": \"568e2bf0-d6f2-97f8-9f15-a57b11dc6977\"\n}\n```\n\n### curl\n\n通过System Message的`text`参数，可以配置Context进行定制化识别。\n\n```\ncurl --location --request POST 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header 'Authorization: Bearer $DASHSCOPE_API_KEY' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"model\": \"qwen3-asr-flash\",\n    \"input\": {\n        \"messages\": [\n            {\n                \"content\": [\n                    {\n                        \"text\": \"\"\n                    }\n                ],\n                \"role\": \"system\"\n            },\n            {\n                \"content\": [\n                    {\n                        \"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"\n                    }\n                ],\n                \"role\": \"user\"\n            }\n        ]\n    },\n    \"parameters\": {\n        \"asr_options\": {\n            \"enable_lid\": true,\n            \"enable_itn\": false\n        }\n    }\n}'\n```\n\n完整结果以JSON格式输出到控制台。完整结果包含状态码、唯一的请求ID、识别后的内容以及本次调用的token信息。\n\n```\n{\n    \"output\": {\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"message\": {\n                    \"annotations\": [\n                        {\n                            \"language\": \"zh\",\n                            \"type\": \"audio_info\"\n                        }\n                    ],\n                    \"content\": [\n                        {\n                            \"text\": \"欢迎使用阿里云。\"\n                        }\n                    ],\n                    \"role\": \"assistant\"\n                }\n            }\n        ]\n    },\n    \"usage\": {\n        \"input_tokens_details\": {\n            \"text_tokens\": 0\n        },\n        \"output_tokens_details\": {\n            \"text_tokens\": 6\n        },\n        \"seconds\": 1\n    },\n    \"request_id\": \"568e2bf0-d6f2-97f8-9f15-a57b11dc6977\"\n}\n```\n\n## 联网搜索\n\n## Python\n\n```\nimport os\nimport dashscope\n\nmessages = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'},\n    {'role': 'user', 'content': '杭州明天天气是什么？'}\n    ]\nresponse = dashscope.Generation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv('DASHSCOPE_API_KEY'),\n    model=\"qwen-plus\", # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages,\n    enable_search=True,\n    result_format='message'\n    )\nprint(response)\n```\n\n## Java\n\n```\n// 建议dashscope SDK的版本 >= 2.12.0\nimport java.util.Arrays;\nimport java.lang.System;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.utils.JsonUtils;\n\npublic class Main {\n    public static GenerationResult callWithMessage() throws ApiException, NoApiKeyException, InputRequiredException {\n        Generation gen = new Generation();\n        Message systemMsg = Message.builder()\n                .role(Role.SYSTEM.getValue())\n                .content(\"You are a helpful assistant.\")\n                .build();\n        Message userMsg = Message.builder()\n                .role(Role.USER.getValue())\n                .content(\"明天杭州什么天气？\")\n                .build();\n        GenerationParam param = GenerationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n                .model(\"qwen-plus\")\n                .messages(Arrays.asList(systemMsg, userMsg))\n                .resultFormat(GenerationParam.ResultFormat.MESSAGE)\n                .enableSearch(true)\n                .build();\n        return gen.call(param);\n    }\n    public static void main(String[] args) {\n        try {\n            GenerationResult result = callWithMessage();\n            System.out.println(JsonUtils.toJson(result));\n        } catch (ApiException | NoApiKeyException | InputRequiredException e) {\n            // 使用日志框架记录异常信息\n            System.err.println(\"An error occurred while calling the generation service: \" + e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n## curl\n\n```\ncurl -X POST https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"qwen-plus\",\n    \"input\":{\n        \"messages\":[      \n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"明天杭州天气如何？\"\n            }\n        ]\n    },\n    \"parameters\": {\n        \"enable_search\": true,\n        \"result_format\": \"message\"\n    }\n}'\n```\n\n## 工具调用\n\n> 完整的Function Calling 流程代码请参见[Function Calling](https://help.aliyun.com/zh/model-studio/qwen-function-calling#0f0fcbd808d8o)。\n\n## Python\n\n```\nimport os\nimport dashscope\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_time\",\n            \"description\": \"当你想知道现在的时间时非常有用。\",\n            \"parameters\": {}\n        }\n    },  \n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"当你想查询指定城市的天气时非常有用。\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n                    }\n                }\n            },\n            \"required\": [\n                \"location\"\n            ]\n        }\n    }\n]\nmessages = [{\"role\": \"user\", \"content\": \"杭州天气怎么样\"}]\nresponse = dashscope.Generation.call(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv('DASHSCOPE_API_KEY'),\n    model='qwen-plus',  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    messages=messages,\n    tools=tools,\n    result_format='message'\n)\nprint(response)\n```\n\n## Java\n\n```\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport com.alibaba.dashscope.aigc.conversation.ConversationParam.ResultFormat;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.tools.FunctionDefinition;\nimport com.alibaba.dashscope.tools.ToolFunction;\nimport com.alibaba.dashscope.utils.JsonUtils;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\npublic class Main {\n    public class GetWeatherTool {\n        private String location;\n        public GetWeatherTool(String location) {\n            this.location = location;\n        }\n        public String call() {\n            return location+\"今天是晴天\";\n        }\n    }\n    public class GetTimeTool {\n        public GetTimeTool() {\n        }\n        public String call() {\n            LocalDateTime now = LocalDateTime.now();\n            DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\");\n            String currentTime = \"当前时间：\" + now.format(formatter) + \"。\";\n            return currentTime;\n        }\n    }\n    public static void SelectTool()\n            throws NoApiKeyException, ApiException, InputRequiredException {\n        SchemaGeneratorConfigBuilder configBuilder =\n                new SchemaGeneratorConfigBuilder(SchemaVersion.DRAFT_2020_12, OptionPreset.PLAIN_JSON);\n        SchemaGeneratorConfig config = configBuilder.with(Option.EXTRA_OPEN_API_FORMAT_VALUES)\n                .without(Option.FLATTENED_ENUMS_FROM_TOSTRING).build();\n        SchemaGenerator generator = new SchemaGenerator(config);\n        ObjectNode jsonSchema_weather = generator.generateSchema(GetWeatherTool.class);\n        ObjectNode jsonSchema_time = generator.generateSchema(GetTimeTool.class);\n        FunctionDefinition fdWeather = FunctionDefinition.builder().name(\"get_current_weather\").description(\"获取指定地区的天气\")\n                .parameters(JsonUtils.parseString(jsonSchema_weather.toString()).getAsJsonObject()).build();\n        FunctionDefinition fdTime = FunctionDefinition.builder().name(\"get_current_time\").description(\"获取当前时刻的时间\")\n                .parameters(JsonUtils.parseString(jsonSchema_time.toString()).getAsJsonObject()).build();\n        Message systemMsg = Message.builder().role(Role.SYSTEM.getValue())\n                .content(\"You are a helpful assistant. When asked a question, use tools wherever possible.\")\n                .build();\n        Message userMsg = Message.builder().role(Role.USER.getValue()).content(\"杭州天气\").build();\n        List<Message> messages = new ArrayList<>();\n        messages.addAll(Arrays.asList(systemMsg, userMsg));\n        GenerationParam param = GenerationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                // 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n                .model(\"qwen-plus\")\n                .messages(messages)\n                .resultFormat(ResultFormat.MESSAGE)\n                .tools(Arrays.asList(\n                        ToolFunction.builder().function(fdWeather).build(),\n                        ToolFunction.builder().function(fdTime).build()))\n                .build();\n        Generation gen = new Generation();\n        GenerationResult result = gen.call(param);\n        System.out.println(JsonUtils.toJson(result));\n    }\n    public static void main(String[] args) {\n        try {\n            SelectTool();\n        } catch (ApiException | NoApiKeyException | InputRequiredException e) {\n            System.out.println(String.format(\"Exception %s\", e.getMessage()));\n        }\n        System.exit(0);\n    }\n}\n```\n\n## curl\n\n```\ncurl --location \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model\": \"qwen-plus\",\n    \"input\": {\n        \"messages\": [{\n            \"role\": \"user\",\n            \"content\": \"杭州天气怎么样\"\n        }]\n    },\n    \"parameters\": {\n        \"result_format\": \"message\",\n        \"tools\": [{\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_time\",\n                \"description\": \"当你想知道现在的时间时非常有用。\",\n                \"parameters\": {}\n            }\n        },{\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"当你想查询指定城市的天气时非常有用。\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n                        }\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }]\n    }\n}'\n```\n\n## 异步调用\n\n```\n# 您的Dashscope Python SDK版本需要不低于 1.19.0。\nimport asyncio\nimport platform\nimport os\nfrom dashscope.aigc.generation import AioGeneration\n\nasync def main():\n    response = await AioGeneration.call(\n        # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n        api_key=os.getenv('DASHSCOPE_API_KEY'),\n        model=\"qwen-plus\",  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n        messages=[{\"role\": \"user\", \"content\": \"你是谁\"}],\n        result_format=\"message\",\n    )\n    print(response)\n\nif platform.system() == \"Windows\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncio.run(main())\n```\n\n## 文字提取\n\n> 关于通义千问OCR模型进行文字提取更多用法，请参见[文字提取](https://help.aliyun.com/zh/model-studio/qwen-vl-ocr)。\n\n## Python\n\n```\n# use [pip install -U dashscope] to update sdk\n\nimport os\nfrom dashscope import MultiModalConversation\n\nmessages = [\n      {\n        \"role\":\"user\",\n        \"content\":[\n          {\n              \"image\":\"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n              \"min_pixels\": 3136,\n              \"max_pixels\": 6422528,\n              \"enable_rotate\": True\n          },\n          {\n          # 当ocr_options中的task字段设置为信息抽取时，模型会以下面text字段中的内容作为Prompt，不支持用户自定义\n              \"text\":\"假设你是一名信息提取专家。现在给你一个JSON模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的JSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用null填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的JSON模式内容如下: {result_schema}。\"\n          }\n        ]\n      }\n    ]\nparams = {\n  \"ocr_options\":{\n    \"task\": \"key_information_extraction\",\n    \"task_config\": {\n      \"result_schema\": {\n          \"销售方名称\": \"\",\n          \"购买方名称\": \"\",\n          \"不含税价\": \"\",\n          \"组织机构代码\": \"\",\n          \"发票代码\": \"\"\n      }\n    }\n  }\n}\n\nresponse = MultiModalConversation.call(model='qwen-vl-ocr-latest',\n                                       messages=messages,\n                                       **params,\n                                       api_key=os.getenv('DASHSCOPE_API_KEY'))\n\nprint(response.output.choices[0].message.content[0][\"ocr_result\"])\n```\n\n## Java\n\n```\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Map;\nimport java.util.HashMap;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.aigc.multimodalconversation.OcrOptions;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.google.gson.JsonObject;\n\npublic class Main {\n    public static void simpleMultiModalConversationCall()\n            throws ApiException, NoApiKeyException, UploadFileException {\n        MultiModalConversation conv = new MultiModalConversation();\n        Map<String, Object> map = new HashMap<>();\n        map.put(\"image\", \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\");\n        // 输入图像的最大像素阈值，超过该值图像会按原比例缩小，直到总像素低于max_pixels\n        map.put(\"max_pixels\", \"6422528\");\n        // 输入图像的最小像素阈值，小于该值图像会按原比例放大，直到总像素大于min_pixels\n        map.put(\"min_pixels\", \"3136\");\n        // 开启图像自动转正功能\n        map.put(\"enable_rotate\", true);\n\n        MultiModalMessage userMessage = MultiModalMessage.builder().role(Role.USER.getValue())\n                .content(Arrays.asList(\n                        map,\n                        // 当ocr_options中的task字段设置为信息抽取时，模型会以下面text字段中的内容作为Prompt，不支持用户自定义\n                        Collections.singletonMap(\"text\", \"假设你是一名信息提取专家。现在给你一个JSON模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的JSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用null填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的JSON模式内容如下: {result_schema}。\"))).build();\n\n        // 创建主JSON对象\n        JsonObject resultSchema = new JsonObject();\n        resultSchema.addProperty(\"销售方名称\", \"\");\n        resultSchema.addProperty(\"购买方名称\", \"\");\n        resultSchema.addProperty(\"不含税价\", \"\");\n        resultSchema.addProperty(\"组织机构代码\", \"\");\n        resultSchema.addProperty(\"发票代码\", \"\");\n\n        // 配置内置的OCR任务\n        OcrOptions ocrOptions = OcrOptions.builder()\n                .task(OcrOptions.Task.KEY_INFORMATION_EXTRACTION)\n                .taskConfig(OcrOptions.TaskConfig.builder()\n                        .resultSchema(resultSchema)\n                        .build())\n                .build();\n\n        MultiModalConversationParam param = MultiModalConversationParam.builder()\n                // 若没有配置环境变量，请用百炼API Key将下行替换为：.apiKey(\"sk-xxx\")\n                .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n                .model(\"qwen-vl-ocr-latest\")\n                .message(userMessage)\n                .ocrOptions(ocrOptions)\n                .build();\n        MultiModalConversationResult result = conv.call(param);\n        System.out.println(result.getOutput().getChoices().get(0).getMessage().getContent().get(0).get(\"ocr_result\"));\n    }\n\n    public static void main(String[] args) {\n        try {\n            simpleMultiModalConversationCall();\n        } catch (ApiException | NoApiKeyException | UploadFileException e) {\n            System.out.println(e.getMessage());\n        }\n        System.exit(0);\n    }\n}\n```\n\n## curl\n\n```\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '\n{\n  \"model\": \"qwen-vl-ocr-latest\",\n  \"input\": {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"image\": \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n            \"min_pixels\": 3136,\n            \"max_pixels\": 6422528,\n            \"enable_rotate\": true\n          },\n          {\n            \"text\": \"假设你是一名信息提取专家。现在给你一个JSON模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的JSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用null填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的JSON模式内容如下: {result_schema}。\"\n          }\n        ]\n      }\n    ]\n  },\n  \"parameters\": {\n    \"ocr_options\": {\n      \"task\": \"key_information_extraction\",\n    \"task_config\": {\n      \"result_schema\": {\n          \"销售方名称\": \"\",\n          \"购买方名称\": \"\",\n          \"不含税价\": \"\",\n          \"组织机构代码\": \"\",\n          \"发票代码\": \"\"\n      }\n    }\n\n    }\n  }\n}\n'\n```\n\n## 深入研究\n\n> 关于深入研究模型（qwen-deep-research）的更多用法，请参见[深入研究（Qwen-Deep-Research）](https://help.aliyun.com/zh/model-studio/qwen-deep-research)。\n\nPython\n\n```\nimport os\nimport dashscope\n\n# 配置API Key\n# 若没有配置环境变量，请用百炼API Key将下行替换为：API_KEY = \"sk-xxx\"\nAPI_KEY = os.getenv('DASHSCOPE_API_KEY')\n\ndef call_deep_research_model(messages, step_name):\n    print(f\"\\n=== {step_name} ===\")\n    \n    try:\n        responses = dashscope.Generation.call(\n            api_key=API_KEY,\n            model=\"qwen-deep-research\",\n            messages=messages,\n            # qwen-deep-research模型目前仅支持流式输出\n            stream=True\n            # incremental_output=True 使用增量输出请添加此参数\n        )\n        \n        return process_responses(responses, step_name)\n        \n    except Exception as e:\n        print(f\"调用API时发生错误: {e}\")\n        return \"\"\n\n\n# 显示阶段内容\ndef display_phase_content(phase, content, status):\n    if content:\n        print(f\"\\n[{phase}] {status}: {content}\")\n    else:\n        print(f\"\\n[{phase}] {status}\")\n\n# 处理响应\ndef process_responses(responses, step_name):\n    current_phase = None\n    phase_content = \"\"\n    research_goal = \"\"\n    web_sites = []\n    keepalive_shown = False  # 标记是否已经显示过KeepAlive提示\n\n    for response in responses:\n        # 检查响应状态码\n        if hasattr(response, 'status_code') and response.status_code != 200:\n            print(f\"HTTP返回码：{response.status_code}\")\n            if hasattr(response, 'code'):\n                print(f\"错误码：{response.code}\")\n            if hasattr(response, 'message'):\n                print(f\"错误信息：{response.message}\")\n            print(\"请参考文档：https://help.aliyun.com/zh/model-studio/developer-reference/error-code\")\n            continue\n\n        if hasattr(response, 'output') and response.output:\n            message = response.output.get('message', {})\n            phase = message.get('phase')\n            content = message.get('content', '')\n            status = message.get('status')\n            extra = message.get('extra', {})\n\n            # 阶段变化检测\n            if phase != current_phase:\n                if current_phase and phase_content:\n                    # 根据阶段名称和步骤名称来显示不同的完成描述\n                    if step_name == \"第一步：模型反问确认\" and current_phase == \"answer\":\n                        print(f\"\\n 模型反问阶段完成\")\n                    else:\n                        print(f\"\\n {current_phase} 阶段完成\")\n                current_phase = phase\n                phase_content = \"\"\n                keepalive_shown = False  # 重置KeepAlive提示标记\n\n                # 根据阶段名称和步骤名称来显示不同的描述\n                if step_name == \"第一步：模型反问确认\" and phase == \"answer\":\n                    print(f\"\\n 进入模型反问阶段\")\n                else:\n                    print(f\"\\n 进入 {phase} 阶段\")\n\n            # 处理WebResearch阶段的特殊信息\n            if phase == \"WebResearch\":\n                if extra.get('deep_research', {}).get('research'):\n                    research_info = extra['deep_research']['research']\n\n                    # 处理streamingQueries状态\n                    if status == \"streamingQueries\":\n                        if 'researchGoal' in research_info:\n                            goal = research_info['researchGoal']\n                            if goal:\n                                research_goal += goal\n                                print(f\"\\n   研究目标: {goal}\", end='', flush=True)\n\n                    # 处理streamingWebResult状态\n                    elif status == \"streamingWebResult\":\n                        if 'webSites' in research_info:\n                            sites = research_info['webSites']\n                            if sites and sites != web_sites:  # 避免重复显示\n                                web_sites = sites\n                                print(f\"\\n   找到 {len(sites)} 个相关网站:\")\n                                for i, site in enumerate(sites, 1):\n                                    print(f\"     {i}. {site.get('title', '无标题')}\")\n                                    print(f\"        描述: {site.get('description', '无描述')[:100]}...\")\n                                    print(f\"        URL: {site.get('url', '无链接')}\")\n                                    if site.get('favicon'):\n                                        print(f\"        图标: {site['favicon']}\")\n                                    print()\n\n                    # 处理WebResultFinished状态\n                    elif status == \"WebResultFinished\":\n                        print(f\"\\n   网络搜索完成，共找到 {len(web_sites)} 个参考信息源\")\n                        if research_goal:\n                            print(f\"   研究目标: {research_goal}\")\n\n            # 累积内容并显示\n            if content:\n                phase_content += content\n                # 实时显示内容\n                print(content, end='', flush=True)\n\n            # 显示阶段状态变化\n            if status and status != \"typing\":\n                print(f\"\\n   状态: {status}\")\n\n                # 显示状态说明\n                if status == \"streamingQueries\":\n                    print(\"   → 正在生成研究目标和搜索查询（WebResearch阶段）\")\n                elif status == \"streamingWebResult\":\n                    print(\"   → 正在执行搜索、网页阅读和代码执行（WebResearch阶段）\")\n                elif status == \"WebResultFinished\":\n                    print(\"   → 网络搜索阶段完成（WebResearch阶段）\")\n\n            # 当状态为finished时，显示token消耗情况\n            if status == \"finished\":\n                if hasattr(response, 'usage') and response.usage:\n                    usage = response.usage\n                    print(f\"\\n    Token消耗统计:\")\n                    print(f\"      输入tokens: {usage.get('input_tokens', 0)}\")\n                    print(f\"      输出tokens: {usage.get('output_tokens', 0)}\")\n                    print(f\"      请求ID: {response.get('request_id', '未知')}\")\n\n            if phase == \"KeepAlive\":\n                # 只在第一次进入KeepAlive阶段时显示提示\n                if not keepalive_shown:\n                    print(\"当前步骤已经完成，准备开始下一步骤工作\")\n                    keepalive_shown = True\n                continue\n\n    if current_phase and phase_content:\n        if step_name == \"第一步：模型反问确认\" and current_phase == \"answer\":\n            print(f\"\\n 模型反问阶段完成\")\n        else:\n            print(f\"\\n {current_phase} 阶段完成\")\n\n    return phase_content\n\ndef main():\n    # 检查API Key\n    if not API_KEY:\n        print(\"错误：未设置 DASHSCOPE_API_KEY 环境变量\")\n        print(\"请设置环境变量或直接在代码中修改 API_KEY 变量\")\n        return\n    \n    print(\"用户发起对话：研究一下人工智能在教育中的应用\")\n    \n    # 第一步：模型反问确认\n    # 模型会分析用户问题，提出细化问题来明确研究方向\n    messages = [{'role': 'user', 'content': '研究一下人工智能在教育中的应用'}]\n    step1_content = call_deep_research_model(messages, \"第一步：模型反问确认\")\n\n    # 第二步：深入研究\n    # 基于第一步的反问内容，模型会执行完整的研究流程\n    messages = [\n        {'role': 'user', 'content': '研究一下人工智能在教育中的应用'},\n        {'role': 'assistant', 'content': step1_content},  # 包含模型的反问内容\n        {'role': 'user', 'content': '我主要关注个性化学习和智能评估这两个方面'}\n    ]\n    \n    call_deep_research_model(messages, \"第二步：深入研究\")\n    print(\"\\n 研究完成！\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\ncurl\n\n```\necho \"第一步：模型反问确认\"\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation' \\\n--header 'X-DashScope-SSE: enable' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"input\": {\n        \"messages\": [\n            {\n                \"content\": \"研究一下人工智能在教育中的应用\", \n                \"role\": \"user\"\n            }\n        ]\n    },\n    \"model\": \"qwen-deep-research\"\n}'\n\necho -e \"\\n\\n\" \necho \"第二步：深入研究\"\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation' \\\n--header 'X-DashScope-SSE: enable' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"input\": {\n        \"messages\": [\n            {\n                \"content\": \"研究一下人工智能在教育中的应用\", \n                \"role\": \"user\"\n            },\n            {\n                \"content\": \"请告诉我您希望重点研究人工智能在教育中的哪些具体应用场景？\", \n                \"role\": \"assistant\"\n            },\n            {\n                \"content\": \"我主要关注个性化学习方面\", \n                \"role\": \"user\"\n            }\n        ]\n    },\n    \"model\": \"qwen-deep-research\"\n}'\n```\n\n**model** `*string*` **（必选）**\n\n模型名称。\n\n支持的模型：通义千问大语言模型（商业版、开源版）、代码模型、通义千问VL、深入研究模型、通义千问Audio、数学模型\n\n**具体模型名称和计费，请参见**[模型列表](https://help.aliyun.com/zh/model-studio/models#9f8890ce29g5u)。\n\n**messages** `*array*` **（必选）**\n\n由历史对话组成的消息列表。\n\n> 通过HTTP调用时，请将**messages** 放入 **input** 对象中。\n\n**消息类型**\n\nSystem Message `*object*`（可选）\n\n模型的目标或角色。如果设置系统消息，请放在messages列表的第一位。\n\n**属性**\n\n**content** `*string 或 array*`**（必选）**\n\n消息内容。仅在调用[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)功能时为array类型；其他情况为string类型。\n\n**属性**\n\n**text** `*string*`\n\n指定上下文（Context）。通义千问3 ASR支持用户在语音识别的同时，提供背景文本、实体词表等参考信息（Context），从而获得定制化的识别结果。\n\n长度限制：不超过10000 Token。\n\n具体介绍请参见[上下文增强](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition#33fdf0438d5jd)。\n\n**role** `*string*` **（必选）**\n\n固定为`system`。\n\n> QwQ 模型不建议设置 System Message，QVQ 模型设置System Message不会生效。\n\nUser Message `*object*`**（必选）**\n\n用户发送给模型的消息。\n\n**属性**\n\n**content** `*string 或 array*` **（必选）**\n\n用户消息的内容。如果您的输入只有文本，则为string类型；如果您的输入包含图像等多模态数据，则为array类型。\n\n**属性**\n\n**text** `*string*`\n\n传入的文本信息。\n\n**image** `*string*`\n\n使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)进行图片理解时，传入的图片文件。可以为图片的URL或本地路径信息。传入本地文件请参见[本地文件（Qwen-VL）](https://help.aliyun.com/zh/model-studio/vision#f18fc2bb52wxo)或[本地文件（QVQ）](https://help.aliyun.com/zh/model-studio/qvq#f18fc2bb52wxo)。\n\n示例值：{\"image\":\"https://xxxx.jpeg\"}\n\n**enable\\_rotate** `*boolean*` **（可选）**\n\n使用通义千问OCR模型进行文字提取前对图像进行自动转正。\n\n与image参数一起使用，默认值：false。\n\n示例值：{\"image\":\"https://xxxx.jpeg\", \"enable\\_rotate\":False}\n\n**video** `*array或string*`\n\n使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)或[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)进行视频理解时传入的视频文件，如果传入的是图像列表，则为`*array*`类型，如果传入的是视频文件，则为`*string*`类型*。*传入本地文件请参见[本地文件（Qwen-VL）](https://help.aliyun.com/zh/model-studio/vision#f18fc2bb52wxo)或[本地文件（QVQ）](https://help.aliyun.com/zh/model-studio/qvq#f18fc2bb52wxo)。\n\n示例值：\n\n-   图像列表：{\"video\":\\[\"https://xx1.jpg\",...,\"https://xxn.jpg\"\\]}\n    \n-   视频文件：{\"video\":\"https://xxx.mp4\"}\n    \n\n> 对于Qwen-VL 模型，仅部分模型可直接传入视频文件，详情请参见[视频理解（Qwen-VL）](https://help.aliyun.com/zh/model-studio/vision#80dbf6ca8fh6s)；对于QVQ模型，可直接传入视频文件。\n\n**fps** `*float*` **（可选）**\n\n-   [Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)或[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)传入视频文件时设置：用于控制抽帧的频率，表示对视频文件每间隔 fps1​秒抽取一帧。\n    \n-   `Qwen2.5-VL`、`Qwen3-VL`模型或[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)传入图像列表时设置：表示图像列表是由原视频每隔fps1​秒抽取的。\n    \n\n另外，fps参数还能使 `Qwen2.5-VL`、`Qwen3-VL`、QVQ模型感知每帧之间的时间间隔；相较于其他Qwen-VL模型，增加了时间维度的理解能力，如对具体事件进行定位或对不同时段进行要点总结等。\n\n**Qwen2.5-VL 系列模型**\n\n-   qwen-vl-max系列：`qwen-vl-max-latest`、qwen-vl-max-2025-01-25及之后的模型\n    \n-   qwen-vl-plus系列：`qwen-vl-plus、``qwen-vl-plus-latest`、`qwen-vl-plus-2025-01-25及之后的模型`\n    \n-   开源系列：`qwen2.5-vl系列模型`\n    \n\n与video参数一起使用，取值范围为 (0.1, 10)，默认值为2.0，示例值如下：\n\n-   图像列表传入：{\"video\":\\[\"https://xx1.jpg\",...,\"https://xxn.jpg\"\\]，\"fps\":2}\n    \n-   视频文件传入：{\"video\": \"https://xx1.mp4\"，\"fps\":2}\n    \n\n较大的fps适合高速运动的场景（如体育赛事、动作电影等），较小的fps适合长视频或内容偏静态的场景。\n\n> 使用OpenAI SDK时，视频文件默认每间隔0.5秒抽取一帧，图像列表默认是以每隔0.5秒从视频中抽取出来的，且不支持修改。\n\n**audio** `*string*`\n\n> 模型为音频理解或语音识别类模型时，是必选参数，如模型为qwen3-asr-flash、qwen2-audio-instruct等。\n\n使用音频理解或[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)功能时，传入的音频文件。\n\n示例值：{\"audio\":\"https://xxx.mp3\"}\n\n**min\\_pixels** `*integer*` **（可选）**\n\nQwen-OCR、Qwen-VL模型支持，用于设定输入图像的最小像素阈值。\n\n当输入图像像素小于`min_pixels`时，会将图像按原比例放大，直到总像素高于`min_pixels`。\n\n**min\\_pixels 取值范围**\n\n-   Qwen-OCR、`qwen-vl-max-0813及之前`、`qwen-vl-plus-0710`及之前更新的模型：默认值和最小值均为3136\n    \n-   `qwen-vl-max-0813`及之后、`qwen-vl-plus-0710`及以后更新的模型：默认值和最小值均为4096\n    \n-   Qwen3-VL：默认值：65536，最小值：4096\n    \n\n**max\\_pixels** `*integer*` **（可选）**\n\nQwen-OCR、Qwen-VL支持，用于设定输入图像的最大像素阈值。\n\n当输入图像像素在`[min_pixels, max_pixels]`区间内时，模型会按原图进行识别。当输入图像像素大于`max_pixels`时，会将图像按原比例缩小，直到总像素低于`max_pixels`。\n\n**max\\_pixels 取值范围**\n\n-   对于Qwen-OCR模型：默认值：6422528，最大值：23520000\n    \n-   对于Qwen-VL模型，分两种情况：\n    \n    -   当[vl\\_high\\_resolution\\_images](#f49c99a427jh3)为False（关闭高分辨率模式）时：\n        \n        -   Qwen2.5-VL：默认值和最大值均为1003520\n            \n        -   Qwen3-VL：默认值和最大值均为：2621440\n            \n    -   当[vl\\_high\\_resolution\\_images](#f49c99a427jh3)为True（开启高分辨率模式）时：\n        \n        -   Qwen2.5-VL：固定为 12845056\n            \n        -   Qwen3-VL：固定为 16777216\n            \n\n**cache\\_control** `*object*` **（可选）**\n\n仅支持[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)的模型支持，用于开启显式缓存。\n\n**属性**\n\n**type** `*string*`**（必选）**\n\n固定为`ephemeral`。\n\n**role** `*string*` **（必选）**\n\n用户消息的角色，固定为`user`。\n\nAssistant Message `*object*` （可选）\n\n模型对用户消息的回复。\n\n**属性**\n\n**content** `*string*` （可选）\n\n消息内容。仅当助手消息中指定`tool_calls`参数时非必选。\n\n**role** `*string*` **（必选）**\n\n固定为`assistant`。\n\n**partial** `*boolean*` （可选）\n\n是否开启Partial Mode。使用方法请参考[前缀续写](https://help.aliyun.com/zh/model-studio/partial-mode)。\n\n**支持的模型**\n\n-   **通义千问Max 系列**\n    \n    qwen-max、qwen-max-latest、qwen-max-2024-09-19及之后的快照模型\n    \n-   **通义千问Plus 系列（非思考模式）**\n    \n    qwen-plus、qwen-plus-latest、qwen-plus-2024-09-19及之后的快照模型\n    \n-   **通义千问Flash 系列（非思考模式）**\n    \n    qwen-flash、qwen-flash-2025-07-28及之后的快照模型\n    \n-   **通义千问Coder 系列**\n    \n    qwen3-coder-plus、qwen3-coder-flash、qwen3-coder-480b-a35b-instruct、qwen3-coder-30b-a3b-instruct、qwen-coder-plus、qwen-coder-plus-latest、qwen-coder-plus-2024-11-06、qwen-coder-turbo、qwen-coder-turbo-latest、qwen-coder-turbo-2024-09-19、qwen2.5-coder-32b-instruct、qwen2.5-coder-14b-instruct、qwen2.5-coder-7b-instruct、qwen2.5-coder-3b-instruct、qwen2.5-coder-1.5b-instruct、qwen2.5-coder-0.5b-instruct\n    \n-   **通义千问VL 系列**\n    \n    -   **qwen-vl-max 系列**\n        \n        qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2024-08-09及之后的快照模型\n        \n    -   **qwen-vl-plus 系列**\n        \n        qwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2024-08-09及之后的快照模型\n        \n-   **通义千问Turbo 系列（非思考模式）**\n    \n    qwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19及之后的快照模型\n    \n-   **通义千问开源系列**\n    \n    Qwen3 开源模型（非思考模式）、qwen2.5-72b-instruct、qwen2.5-32b-instruct、qwen2.5-14b-instruct、qwen2.5-7b-instruct、qwen2.5-3b-instruct、qwen2.5-1.5b-instruct、qwen2.5-0.5b-instruct\n    \n-   **通义千问Math 系列**\n    \n    qwen-math-plus、qwen-math-plus-latest、qwen-math-plus-0919、qwen-math-turbo、qwen-math-turbo-latest、qwen-math-turbo-0919、qwen2.5-math-72b-instruct、qwen2.5-math-7b-instruct、qwen2.5-math-1.5b-instruct\n    \n\n**tool\\_calls** `*array*` （可选）\n\n在发起 Function Calling后，模型回复的要调用的工具和调用工具时需要的参数。包含一个或多个对象。由上一轮模型响应的`tool_calls`字段获得。\n\n**属性**\n\n**id** `*string*`\n\n本次工具响应的ID。\n\n**type** `*string*`\n\n工具的类型，当前只支持`function`。\n\n**function** `*object*`\n\n需要被调用的函数。\n\n**属性**\n\n**name** `*string*`\n\n需要被调用的函数名。\n\n**arguments** `*string*`\n\n需要输入到工具中的参数，为JSON字符串。\n\n**index** `*integer*`\n\n工具信息在`tool_calls`列表中的索引。\n\nTool Message `*object*`（可选）\n\n工具的输出信息。\n\n**属性**\n\n**content** `*string*` **（必选）**\n\n工具消息的内容，一般为工具函数的输出。\n\n**role** `*string*` **（必选）**\n\n工具消息的角色，固定为`tool`。\n\n**tool\\_call\\_id** `*string*` **（可选）**\n\n发起 Function Calling 后返回的 id，可以通过`response.output.choices[0].message.tool_calls[0][\"id\"]`获取，用于标记 Tool Message 对应的工具。\n\n**temperature** `*float*` （可选）\n\n采样温度，控制模型生成文本的多样性。\n\ntemperature越高，生成的文本更多样，反之，生成的文本更确定。\n\n取值范围： \\[0, 2)\n\n**temperature默认值**\n\n-   Qwen3（非思考模式）、Qwen3-Instruct系列、Qwen3-Coder系列、qwen-max系列、qwen-plus系列（非思考模式）、qwen-flash系列（非思考模式）、qwen-turbo系列（非思考模式）、qwen开源系列、qwen-coder系列、qwen2-audio-instruct、qwen-doc-turbo、qwen-vl-max-2025-08-13、Qwen3-VL（非思考）：0.7；\n    \n-   QVQ系列 、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15 : 0.5；\n    \n-   qwen-audio-asr系列、qwen-audio-turbo系列：0.00001；\n    \n-   qwen-vl系列、qwen-vl-ocr系列、qwen2.5-omni-7b、qvq-72b-preview：0.01；\n    \n-   qwen-math系列：0；\n    \n-   Qwen3（思考模式）、Qwen3-Thinking、Qwen3-Omni-Captioner、QwQ 系列：0.6；\n    \n-   qwen-long系列： 1.0；\n    \n-   qwen-plus-character：0.92\n    \n-   qwen3-omni-flash系列：0.9\n    \n-   Qwen3-VL（思考模式）：0.8\n    \n\n> 通过HTTP调用时，请将 **temperature** 放入 **parameters** 对象中。\n\n> 不建议修改QVQ模型的默认 temperature 值。\n\n**top\\_p** `*float*` （可选）\n\n核采样的概率阈值，控制模型生成文本的多样性。\n\ntop\\_p越高，生成的文本更多样。反之，生成的文本更确定。\n\n取值范围：（0,1.0\\]。\n\n**top\\_p默认值**\n\nQwen3（非思考模式）、Qwen3-Instruct系列、Qwen3-Coder系列、qwen-max系列、qwen-plus系列（非思考模式）、qwen-flash系列（非思考模式）、qwen-turbo系列（非思考模式）、qwen开源系列、qwen-coder系列、qwen-long、qwen-doc-turbo、qwq-32b-preview、qwen-audio-asr系列、qwen-audio-turbo系列、qwen-vl-max-2025-08-13、Qwen3-VL（非思考模式）：0.8；\n\nqwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen2-vl-72b-instruct、qwen-omni-turbo 系列：0.01；\n\nqwen-vl-plus系列、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08、qwen-vl-max-2025-04-02、qwen-vl-max-2025-01-25、qwen-vl-max-2024-12-30、qvq-72b-preview、qwen2-vl-2b-instruct、qwen2-vl-7b-instruct、qwen2.5-vl-3b-instruct、qwen2.5-vl-7b-instruct、qwen2.5-vl-32b-instruct、qwen2.5-vl-72b-instruct、qwen-vl-ocr系列：0.001；\n\nQVQ系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15 、qwen2-audio-instruct：0.5；\n\nqwen-math系列、Qwen3-Omni-Flash系列：1.0；\n\nQwen3（思考模式）、Qwen3-VL（思考模式）、Qwen3-Thinking、QwQ 系列、Qwen3-Omni-Captioner、qwen-plus-character：0.95\n\n> Java SDK中为**topP***。*通过HTTP调用时，请将 **top\\_p** 放入 **parameters** 对象中。\n\n> 不建议修改QVQ模型的默认 top\\_p 值。\n\n**top\\_k** `*integer*` （可选）\n\n生成过程中采样候选集的大小。例如，取值为50时，仅将单次生成中得分最高的50个Token组成随机采样的候选集。取值越大，生成的随机性越高；取值越小，生成的确定性越高。取值为None或当top\\_k大于100时，表示不启用top\\_k策略，此时仅有top\\_p策略生效。\n\n取值需要大于或等于0。\n\n**top\\_k默认值**\n\nQVQ系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15：10；\n\nQwQ 系列：40；\n\nqwen-math 系列、其余qwen-vl-plus系列、qwen-vl-max-2025-08-13之前的模型、qwen-vl-ocr系列、qwen-audio-asr系列、qwen-audio-turbo系列、qwen2.5-omni-7b、qvq-72b-preview：1；\n\nQwen3-Omni-Flash系列：50\n\n其余模型均为20；\n\n> Java SDK中为**topK***。*通过HTTP调用时，请将 **top\\_k** 放入 **parameters** 对象中。\n\n> 不建议修改QVQ模型的默认 top\\_k 值。\n\n**enable\\_thinking** `*boolean*` （可选）\n\n是否开启思考模式，适用于 Qwen3 、Qwen3-VL商业版与开源版、Qwen3-Omni-Flash模型\n\nQwen3 开源版默认值为 True，Qwen3 商业版模型默认值为 False。\n\n> Java SDK 为enableThinking；通过HTTP调用时，请将 **enable\\_thinking** 放入 **parameters** 对象中。\n\n**thinking\\_budget** `*integer*` （可选）\n\n思考过程的最大长度，在`enable_thinking`为`true`时生效，适用于 Qwen3 全系、Qwen3-VL模型。详情请参见[限制思考长度](https://help.aliyun.com/zh/model-studio/deep-thinking#e7c0002fe4meu)。\n\n> 默认值为模型最大思维链长度。\n\n**repetition\\_penalty** `*float*` （可选）\n\n模型生成时连续序列中的重复度。提高repetition\\_penalty时可以降低模型生成的重复度，1.0表示不做惩罚。没有严格的取值范围，只要大于0即可。\n\n**repetition\\_penalty默认值**\n\n-   qwen-max、qwen-max-latest、qwen-max-2024-09-19、qwen-math系列、qwen-vl-max系列、qvq-72b-preview、qwen2-vl-72b-instruct、qwen-vl-plus-2025-01-02、qwen-vl-plus-2025-05-07、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15、qwen-vl-plus-latest、qwen2.5-vl-3b-instruct、qwen2.5-vl-7b-instruct、qwen2.5-vl-32b-instruct、qwen2.5-vl-72b-instruct、qwen-audio-turbo-latest、qwen-audio-turbo-2024-12-04、QVQ系列、QwQ系列、qwq-32b-preview、Qwen3-VL： 1.0；\n    \n-   qwen-coder系列、qwen2.5-1.5b-instruct、qwen2.5-0.5b-instruct、qwen2-1.5b-instruct、qwen2-0.5b-instruct、qwen2-vl-2b-instruct、qwen2-vl-7b-instruct、qwen-vl-plus-2024-08-09、qwen-vl-plus-2023-12-01、qwen2.5-omni-7b、qwen2-audio-instruct：1.1；\n    \n-   qwen-vl-plus、qwen-vl-plus-2025-01-25：1.2；\n    \n-   其余模型为1.05。\n    \n\n> Java SDK中为**repetitionPenalty***。*通过HTTP调用时，请将 **repetition\\_penalty** 放入 **parameters** 对象中。\n\n> 使用qwen-vl-plus、qwen-vl-plus\\_2025-01-25模型进行文字提取时，建议设置repetition\\_penalty为1.0。\n\n> 对于通义千问OCR模型，repetition\\_penalty的默认值为1.05，该参数对模型效果影响较大，请勿随意修改。\n\n> 不建议修改QVQ模型的默认 repetition\\_penalty 值。\n\n**presence\\_penalty** `*float*` （可选）\n\n控制模型生成文本时的内容重复度。\n\n取值范围：\\[-2.0, 2.0\\]。正数会减少重复度，负数会增加重复度。\n\n适用场景：\n\n较高的presence\\_penalty适用于要求多样性、趣味性或创造性的场景，如创意写作或头脑风暴。\n\n较低的presence\\_penalty适用于要求一致性或专业术语的场景，如技术文档或其他正式文档。\n\n**presence\\_penalty默认值**\n\nQwen3（非思考模式）、Qwen3-Instruct系列、qwen3-0.6b/1.7b/4b（思考模式）、QVQ系列、qwen-max、qwen-max-latest、qwen-max-latest、qwen-max-2024-09-19、qwen2.5-vl系列、qwen-vl-max系列、qwen-vl-plus、qwen2-vl-72b-instruct、qwen-vl-plus-2025-01-02、Qwen3-VL（非思考）：1.5；\n\nqwen-vl-plus-latest、qwen-vl-plus-2025-08-15、qwen-vl-plus-2025-07-10：1.2\n\nqwen-vl-plus-2025-01-25：1.0；\n\nqwen3-8b/14b/32b/30b-a3b/235b-a22b（思考模式）、qwen-plus/qwen-plus-latest/2025-04-28（思考模式）、qwen-turbo/qwen-turbo/2025-04-28（思考模式）：0.5；\n\n其余均为0.0。\n\n**原理介绍**\n\n如果参数值是正数，模型将对目前文本中已存在的Token施加一个惩罚值（惩罚值与文本出现的次数无关），减少这些Token重复出现的几率，从而减少内容重复度，增加用词多样性。\n\n**示例**\n\n提示词：把这句话翻译成中文“This movie is good. The plot is good, the acting is good, the music is good, and overall, the whole movie is just good. It is really good, in fact. The plot is so good, and the acting is so good, and the music is so good.”\n\n参数值为2.0：这部电影很好。剧情很棒，演技棒，音乐也非常好听，总的来说，整部电影都好得不得了。实际上它真的很优秀。剧情非常精彩，演技出色，音乐也是那么的动听。\n\n参数值为0.0：这部电影很好。剧情好，演技好，音乐也好，总的来说，整部电影都很好。事实上，它真的很棒。剧情非常好，演技也非常出色，音乐也同样优秀。\n\n参数值为-2.0：这部电影很好。情节很好，演技很好，音乐也很好，总的来说，整部电影都很好。实际上，它真的很棒。情节非常好，演技也非常好，音乐也非常好。\n\n> 使用qwen-vl-plus-2025-01-25模型进行文字提取时，建议设置presence\\_penalty为1.5。\n\n> 不建议修改QVQ模型的默认presence\\_penalty值。\n\n> Java SDK不支持设置该参数*。*通过HTTP调用时，请将 **presence\\_penalty** 放入 **parameters** 对象中。\n\n**vl\\_high\\_resolution\\_images** `*boolean*` （可选）默认值为 `false`\n\n**vl\\_high\\_resolution\\_images** `*boolean*` （可选）默认值为`false`\n\n是否提高输入图片的默认Token上限，适用于Qwen-VL、QVQ模型。\n\n-   False（默认值）：使用默认的Token处理图像\n    \n    -   `Qwen3-VL商业版及开源版`、`qwen-vl-max-0813`及以后、`qwen-vl-plus-0710`及以后更新的模型：默认Token上限为2560\n        \n    -   QVQ及其他Qwen-VL模型：默认Token上限为1280\n        \n-   True：输入图片的Token上限将提高为16384\n    \n\n**支持的模型**\n\n-   QVQ\n    \n-   Qwen3-VL\n    \n-   qwen-vl-max商业版：qwen-vl-max-0809及之后、qwen-vl-plus-0809及之后的模型\n    \n-   Qwen-VL开源：qwen2-vl、qwen2.5-vl、qwen3-vl\n    \n\n> Java SDK 为 **vlHighResolutionImages**，Java SDK最低版本为2.20.8*。*通过HTTP调用时，请将 **vl\\_high\\_resolution\\_images** 放入 **parameters** 对象中。\n\n**vl\\_enable\\_image\\_hw\\_output** `*boolean*` （可选）默认值为 `false`\n\n是否返回图像缩放后的尺寸。模型会对输入的图像进行缩放处理，配置为 True 时会返回图像缩放后的高度和宽度，开启流式输出时，该信息在最后一个数据块（chunk）中返回。支持[Qwen-VL模型](https://help.aliyun.com/zh/model-studio/vision)。\n\n> Java SDK中为 **vlEnableImageHwOutput**，Java SDK最低版本为2.20.8*。*通过HTTP调用时，请将 **vl\\_enable\\_image\\_hw\\_output** 放入 **parameters** 对象中。\n\n**ocr\\_options** `*object*` （可选）\n\n当您使用通义千问OCR模型执行内置任务时需要配置的参数。\n\n**属性**\n\n**task** `*string*` （必选）\n\n内置任务的名称，可选值如下：\n\n-   \"`text_recognition`\"：通用文字识别\n    \n-   \"`key_information_extraction`\"：信息抽取\n    \n-   \"`document_parsing`\"：文档解析\n    \n-   \"`table_parsing`\"：表格解析\n    \n-   \"`formula_recognition`\"：公式识别\n    \n-   \"`multi_lan`\"：多语言识别\n    \n-   \"`advanced_recognition`\"：高精识别\n    \n\n**task\\_config** `*arrays*` （可选）\n\n当内置任务task为\"key\\_information\\_extraction\"（信息抽取）时使用。\n\n**属性**\n\n**result\\_schema** `*object*` （必选）\n\n表示需要模型抽取的字段，可以是任意形式的JSON结构，最多可嵌套3层JSON 对象。您只需要填写JSON对象的key，value保持为空即可。\n\n示例值：\n\n```\n\"result_schema\" : {\n     \"收件人信息\" : {\n          \"收件人姓名\" : \"\",\n          \"收件人电话号码\" : \"\",\n          \"收件人地址\":\"\"\n     }\n}\n```\n\n> Java SDK为**OcrOptions**，DashScope Python SDK 最低版本为1.22.2， Java SDK 最低版本为2.18.4。\n\n> 通过HTTP调用时，请将 **ocr\\_options** 放入 **parameters** 对象中。\n\n**max\\_input\\_tokens** `*integer*` （可选）\n\n允许输入的最大 Token 长度。目前仅支持qwen-plus-0728/latest模型。\n\n-   qwen-plus-latest 默认值：129,024\n    \n    > 后续默认值可能调整至1,000,000。\n    \n-   qwen-plus-2025-07-28 默认值：1,000,000\n    \n\n> Java SDK 暂不支持该参数。\n\n**max\\_tokens** `*integer*` （可选）\n\n本次请求返回的最大 Token 数。\n\n> `max_tokens` 的设置不会影响大模型的生成过程，如果模型生成的 Token 数超过`max_tokens`，本次请求会返回截断后的内容。\n\n默认值和最大值都是模型的最大输出长度。关于各模型的最大输出长度，请参见[模型列表](https://help.aliyun.com/zh/model-studio/models#9f8890ce29g5u)。\n\nmax\\_tokens参数适用于需要限制字数（如生成摘要、关键词）、控制成本或减少响应时间的场景。\n\n> qwen-vl-ocr、qwen-vl-ocr-latest**、**qwen-vl-ocr-2025-04-13、qwen-vl-ocr-2025-08-28模型的`max_tokens`参数（最大输出长度）默认为 4096，如需提高该参数值（4097~8192范围），请发送邮件至 [modelstudio@service.aliyun.com](mailto:modelstudio@service.aliyun.com)进行申请，并提供以下信息：主账号ID、图像类型（如文档图、电商图、合同等）、模型名称、预计 QPS 和每日请求总数，以及模型输出长度超过4096的请求占比。\n\n> 对于 QwQ、QVQ 与开启思考模式的 Qwen3 模型，`max_tokens`会限制回复内容的长度，不限制深度思考内容的长度。\n\n> Java SDK中为**maxTokens**（模型为通义千问VL/OCR/Audio/ASR时，Java SDK中为**maxLength，**在 2.18.4 版本之后支持也设置为 maxTokens）*。*通过HTTP调用时，请将 **max\\_tokens** 放入 **parameters** 对象中。\n\n**seed** `*integer*` （可选）\n\n设置seed参数会使文本生成过程更具有确定性，通常用于使模型每次运行的结果一致。\n\n在每次模型调用时传入相同的seed值（由您指定），并保持其他参数不变，模型将尽可能返回相同的结果。\n\n取值范围：0到231−1。\n\n**seed默认值**\n\nqwen-vl-plus-2025-01-02、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08、qwen-vl-max-2025-04-02、qwen-vl-max-2024-12-30、qvq-72b-preview、qvq-max系列：3407；\n\nqwen-vl-max-2025-01-25、qwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen-vl-max-2024-02-01、qwen2-vl-72b-instruct、qwen2-vl-2b-instruct、qwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2025-05-07、qwen-vl-plus-2025-01-25、qwen-vl-plus-2024-08-09、qwen-vl-plus-2023-12-01：无默认值；\n\n其余模型均为1234。\n\n> 通过HTTP调用时，请将 **seed** 放入 **parameters** 对象中。\n\n**stream** `*boolean*` （可选）\n\n是否流式输出回复。参数值：\n\n-   false（默认值）：模型生成完所有内容后一次性返回结果。\n    \n-   true：边生成边输出，即每生成一部分内容就立即输出一个片段（chunk）。\n    \n\n> 该参数仅支持Python SDK。通过Java SDK实现流式输出请通过`streamCall`接口调用；通过HTTP实现流式输出请在Header中指定`X-DashScope-SSE`为`enable`。\n\n> Qwen3商业版（思考模式）、Qwen3开源版、QwQ、QVQ只支持流式输出。\n\n**incremental\\_output** `*boolean*` （可选）默认为`false`（Qwen3-Max、Qwen3-VL、[Qwen3 开源版](https://help.aliyun.com/zh/model-studio/models#9d516d17965af)、[QwQ](https://help.aliyun.com/zh/model-studio/deep-thinking) 、[QVQ](https://help.aliyun.com/zh/model-studio/qvq)模型默认值为 `true`）\n\n在流式输出模式下是否开启增量输出。推荐您优先设置为`true`。\n\n参数值：\n\n-   false：每次输出为当前已经生成的整个序列，最后一次输出为生成的完整结果。\n    \n    ```\n    I\n    I like\n    I like apple\n    I like apple.\n    ```\n    \n-   true：增量输出，即后续输出内容不包含已输出的内容。您需要实时地逐个读取这些片段以获得完整的结果。\n    \n    ```\n    I\n    like\n    apple\n    .\n    ```\n    \n\n> Java SDK中为**incrementalOutput***。*通过HTTP调用时，请将 **incremental\\_output** 放入 **parameters** 对象中。\n\n> QwQ 模型与思考模式下的 Qwen3 模型只支持设置为 `true`。由于 Qwen3 商业版模型默认值为`false`，您需要在思考模式下手动设置为 `true`。\n\n> Qwen3 开源版模型不支持设置为 `false`。\n\n**response\\_format** `*object*` （可选） 默认值为`{\"type\": \"text\"}`\n\n返回内容的格式。可选值：`{\"type\": \"text\"}`或`{\"type\": \"json_object\"}`。设置为`{\"type\": \"json_object\"}`时会输出标准格式的JSON字符串。使用方法请参见：[结构化输出](https://help.aliyun.com/zh/model-studio/json-mode)。\n\n> 如果指定该参数为`{\"type\": \"json_object\"}`，您需要在 System Message 或 User Message 中指引模型输出 JSON 格式，如：“请按照json格式输出。”\n\n> Java SDK 中为 `responseFormat`。通过HTTP调用时，请将 **response\\_format** 放入 **parameters** 对象中。\n\n支持的模型\n\n-   **通义千问Max 系列：**qwen3-max、qwen3-max-2025-09-23、qwen3-amx-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19 及之后的快照模型\n    \n-   **通义千问Plus 系列（非思考模式）**：qwen-plus、qwen-plus-latest、qwen-plus-2024-09-19及之后的快照模型\n    \n-   **通义千问Flash 系列（非思考模式）：**qwen-flash、qwen-flash-2025-07-28及之后的快照模型\n    \n-   **通义千问Coder 系列**：qwen3-coder-plus、qwen3-coder-plus-2025-07-22、qwen3-coder-flash、qwen3-coder-flash-2025-07-28\n    \n-   **通义千问VL 系列：**qwen-vl-max、qwen-vl-plus（不包括最新版与快照版模型）\n    \n-   **通义千问Turbo 系列（非思考模式）**：qwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19及之后的快照模型\n    \n-   **Qwen 开源系列**\n    \n    -   Qwen3（非思考模式）\n        \n    -   Qwen3-Coder\n        \n    -   Qwen2.5 系列的文本模型（不含math与coder模型）\n        \n\n**output\\_format** `*string*` （可选）默认值为`\"model_detailed_report\"`\n\n仅当调用通义千问深入研究模型 qwen-deep-research 时，用于规定输出内容格式。\n\n可选值：\n\n-   \"model\\_detailed\\_report\"：详细研究报告，约6000字\n    \n-   \"model\\_summary\\_report\"：摘要研究报告，约1500-2000字\n    \n\n**result\\_format** `*string*` （可选） 默认为`\"text\"`（Qwen3-Max、Qwen3-VL、[QwQ](https://help.aliyun.com/zh/model-studio/deep-thinking) 模型、Qwen3 开源模型与 Qwen-Long 模型默认值为 `\"message\"`）\n\n返回数据的格式。推荐您优先设置为`\"message\"`，可以更方便地进行[多轮对话](https://help.aliyun.com/zh/model-studio/multi-round-conversation)。\n\n> 平台后续将统一将默认值调整为\"message\"。\n\n> Java SDK中为**resultFormat***。*通过HTTP调用时，请将 **result\\_format** 放入 **parameters** 对象中。\n\n> 模型为通义千问VL/QVQ/OCR/Audio/ASR时，设置“`text`”不生效。\n\n> Qwen3-Max、Qwen3-VL、思考模式下的 Qwen3 模型只能设置为`\"message\"`，由于 Qwen3 商业版模型默认值为\"text\"，您需要将其设置为`\"message\"`。\n\n> 如果您使用 Java SDK 调用Qwen3 开源模型，并且传入了 \"text\"，依然会以 \"message\"格式进行返回。\n\n**logprobs** `*boolean*` （可选）默认值为 `false`\n\n是否返回输出 Token 的对数概率，可选值：\n\n-   true\n    \n    返回；\n    \n-   false\n    \n    不返回。\n    \n\n> 思考阶段生成的内容（`reasoning_content`）不会返回对数概率。\n\n> 支持 qwen-plus、qwen-turbo 系列的快照模型（不包含主线模型）与 Qwen3 开源模型。\n\n**top\\_logprobs** `*integer*` （可选）默认值为0\n\n指定在每一步生成时，返回模型最大概率的候选 Token 个数。\n\n取值范围：\\[0,5\\]\n\n仅当 `logprobs` 为 `true` 时生效。\n\n**n** `*integer*` （可选） 默认值为1\n\n生成响应的个数，取值范围是`1-4`。对于需要生成多个响应的场景（如创意写作、广告文案等），可以设置较大的 n 值。\n\n> 当前仅支持 qwen-plus、 [Qwen3（非思考模式）](https://help.aliyun.com/zh/model-studio/deep-thinking#be9890136awsc)、qwen-plus-character 模型，且在传入 tools 参数时固定为1。\n\n> 设置较大的 n 值不会增加输入 Token 消耗，会增加输出 Token 的消耗。\n\n**stop** `*string 或 array*` （可选）\n\n使用stop参数后，当模型生成的文本即将包含指定的字符串或token\\_id时，将自动停止生成。\n\n您可以在stop参数中传入敏感词来控制模型的输出。\n\n> stop为array类型时，不可以将token\\_id和字符串同时作为元素输入，比如不可以指定stop为`[\"你好\",104307]`。\n\n**tools** `*array*` （可选）\n\n可供模型调用的工具数组，可以包含一个或多个工具对象。一次 Function Calling 流程模型会从中选择其中一个工具（开启[parallel\\_tool\\_calls](#5398ced61euxr)参数可能选择多个工具）。使用 tools 时需要同时指定`result_format`参数为`\"message\"`。无论是发起 Function Calling，还是向模型提交工具函数的执行结果，均需设置tools参数。\n\n> 目前不支持通义千问VL/Audio，也不建议用于数学和代码模型（Qwen3-Coder 模型除外）。\n\n**属性**\n\n**type** `*string*` **（必选）**\n\ntools的类型，当前仅支持function。\n\n**function** `*object*` **（必选）**\n\n**属性**\n\n**name** `*string*` **（必选）**\n\n工具函数的名称，必须是字母、数字，可以包含下划线和短划线，最大长度为64。\n\n**description** `*string*` **（必选）**\n\n工具函数的描述，供模型选择何时以及如何调用工具函数。\n\n**parameters** `*objcet*` **（必选）**\n\n工具的参数描述，需要是一个合法的JSON Schema。JSON Schema的描述可以见[链接](https://json-schema.org/understanding-json-schema)。如果parameters参数为空，表示function没有入参。\n\n> 通过HTTP调用时，请将 **tools** 放入 **parameters** JSON 对象中。暂时不支持qwen-vl与qwen-audio系列模型。\n\n**tool\\_choice** `*string 或 object*` （可选）\n\n在使用tools参数时，用于控制模型调用指定工具。有三种取值：\n\n-   `\"none\"`表示不调用工具。tools参数为空时，默认值为`\"none\"`。\n    \n-   `\"auto\"`表示由模型判断是否调用工具，可能调用也可能不调用。tools参数不为空时，默认值为`\"auto\"`。\n    \n-   object结构可以指定模型调用的工具。例如`tool_choice={\"type\": \"function\", \"function\": {\"name\": \"user_function\"}}`。\n    \n    -   type只支持指定为`\"function\"`。\n        \n    -   function\n        \n        -   name表示期望被调用的工具名称，例如`\"get_current_time\"`。\n            \n\n> Java SDK中为**toolChoice***。*通过HTTP调用时，请将 **tool\\_choice** 放入 **parameters** 对象中。\n\n**parallel\\_tool\\_calls** `*boolean*` （可选）默认值为 `false`\n\n是否开启并行工具调用。参数为`true`时开启，为`false`时不开启。并行工具调用详情请参见：[并行工具调用](https://help.aliyun.com/zh/model-studio/qwen-function-calling#cb6b5c484bt4x)。\n\n**translation\\_options** `*object*` （可选）\n\n当您使用[翻译模型](https://help.aliyun.com/zh/model-studio/machine-translation)时需要配置的翻译参数。\n\n**属性**\n\n**source\\_lang** `*string*` （必选）\n\n源语言的英文全称，详情请参见[支持的语言](https://help.aliyun.com/zh/model-studio/machine-translation#038d2865bbydc)。您可以将`source_lang`设置为`\"auto\"`，模型会自动判断输入文本属于哪种语言。\n\n**target\\_lang** `*string*` （必选）\n\n目标语言的英文全称，详情请参见[支持的语言](https://help.aliyun.com/zh/model-studio/machine-translation#038d2865bbydc)。\n\n**terms** `*arrays*` （可选）\n\n在使用[术语干预](https://help.aliyun.com/zh/model-studio/machine-translation#2bf54a5ab5voe)功能时需要设置的术语数组。\n\n**属性**\n\n**source** `*string*` （必选）\n\n源语言的术语。\n\n**target** `*string*` （必选）\n\n目标语言的术语。\n\n**tm\\_list** `*arrays*` （可选）\n\n在[翻译记忆](https://help.aliyun.com/zh/model-studio/machine-translation#17e15234e7gfp)功能时需要设置的翻译记忆数组。\n\n**属性**\n\n**source** `*string*` （必选）\n\n源语言的语句。\n\n**target** `*string*` （必选）\n\n目标语言的语句。\n\n**domains** `*string*` （可选）\n\n在使用[领域提示](https://help.aliyun.com/zh/model-studio/machine-translation#4af23a31db7lf)功能时需要设置的领域提示语句。\n\n> 领域提示语句暂时只支持英文。\n\n> Java SDK中为`translationOptions`。通过HTTP调用时，请将 **translation\\_options** 放入 **parameters** 对象中。\n\n**enable\\_search** `*boolean*` （可选）\n\n模型在生成文本时是否使用互联网搜索结果进行参考。取值如下：\n\n-   true：启用互联网搜索，模型会将搜索结果作为文本生成过程中的参考信息，但模型会基于其内部逻辑判断是否使用互联网搜索结果。\n    \n    > 如果模型没有搜索互联网，建议优化Prompt，或设置`search_options`中的`forced_search`参数开启强制搜索。\n    \n-   false（默认）：关闭互联网搜索。\n    \n\n计费信息请参见[计费说明](https://help.aliyun.com/zh/model-studio/web-search#92ce83df3a599)。\n\n**支持的模型**\n\n-   通义千问Max：qwen3-max、qwen3-max-2025-09-23、qwen3-max-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19及之后的快照版本\n    \n-   通义千问Plus：qwen-plus、qwen-plus-latest、qwen-plus-2025-07-14及之后的快照版本\n    \n-   通义千问Flash：qwen-flash、qwen-flash-2025-07-28及之后的快照版本\n    \n-   通义千问Turbo：qwen-turbo、qwen-turbo-latest、qwen-turbo-2025-07-15\n    \n-   QwQ：qwq-plus（仅支持流式输出）\n    \n-   Kimi：Moonshot-Kimi-K2-Instruct\n    \n\n> Java SDK中为**enableSearch***。*通过HTTP调用时，请将 **enable\\_search** 放入 **parameters** 对象中。\n\n> 启用互联网搜索功能可能会增加 Token 的消耗。\n\n**search\\_options** `*object*` （可选）\n\n联网搜索的策略。仅当`enable_search`为`true`时生效。详情参见[联网搜索](https://help.aliyun.com/zh/model-studio/web-search#cbddf5b28bug8)。\n\n> 通过HTTP调用时，请将 **search\\_options** 放入 **parameters** 对象中。Java SDK中为searchOptions。\n\n**属性**\n\n**enable\\_source** `*boolean*`（可选）默认值为`false`\n\n在返回结果中是否展示搜索到的信息。参数值：\n\n-   true：展示；\n    \n-   false：不展示。\n    \n\n**enable\\_citation** `*boolean*`（可选）默认值为`false`\n\n是否开启\\[1\\]或\\[ref\\_1\\]样式的角标标注功能。在`enable_source`为`true`时生效。参数值：\n\n-   true：开启；\n    \n-   false：不开启。\n    \n\n**citation\\_format** `*string*`（可选）默认值为`\"[<number>]\"`\n\n角标样式。在`enable_citation`为`true`时生效。参数值：\n\n-   \\[<number>\\]：角标形式为`[1]`；\n    \n-   \\[ref\\_<number>\\]：角标形式为`[ref_1]`。\n    \n\n**forced\\_search** `*boolean*`（可选）默认值为`false`\n\n是否强制开启搜索。参数值：\n\n-   true：强制开启；\n    \n-   false：不强制开启。\n    \n\n**search\\_strategy** `*string*`（可选）默认值为`turbo`\n\n搜索互联网信息的策略。参数值：\n\n-   `turbo`：默认策略，兼顾响应速度与搜索效果，推荐使用。\n    \n-   `max`：高性能模式，基于全栈顶配模型与多源搜索引擎，提供最优效果。\n    \n\n**enable\\_search\\_extension** `*boolean*`（可选）默认值为`false`\n\n是否开启特定领域增强。参数值：\n\n-   `true`\n    \n    开启。\n    \n-   `false`（默认值）\n    \n    不开启。\n    \n\n**prepend\\_search\\_result** `*boolean*`（可选）默认值为`false`\n\n在流式输出且`enable_source`为`true`时，可通过`prepend_search_result`配置**第一个返回的数据包**是否只包含搜索来源信息。可选值：\n\n-   `true`\n    \n    只包含搜索来源信息。\n    \n-   `false`（默认值）\n    \n    包含搜索来源信息与大模型回复信息。\n    \n\n> 暂不支持 DashScope Java SDK。\n\n**asr\\_options** `*object*` （可选）\n\n该参数仅在调用[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)功能时可用，且仅对通义千问3 ASR模型生效，用来指定某些功能是否启用。具体用法请参见[快速开始](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition#7818a3bc466d6)。\n\n> 通过HTTP或Java SDK调用时，请将 **asr\\_options** 放入 **parameters** 对象中。\n\n**属性**\n\n**language** `*string*`（可选）无默认值\n\n若已知音频的语种，可通过该参数指定待识别语种，以提升识别准确率。\n\n只能指定一个语种。\n\n若音频语种不确定，或包含多种语种（例如中英日韩混合），请勿指定该参数。\n\n参数值：\n\n-   zh：中文\n    \n-   en：英文\n    \n-   ja：日语\n    \n-   de：德语\n    \n-   ko：韩语\n    \n-   ru：俄语\n    \n-   fr：法语\n    \n-   pt：葡萄牙语\n    \n-   ar：阿拉伯语\n    \n-   it：意大利语\n    \n-   es：西班牙语\n    \n\n**enable\\_itn** `*boolean*`（可选）默认值为`false`\n\n是否启用ITN。该功能仅适用于中文和英文音频。\n\nITN 是 Inverse Text Normalization 的缩写，中文一般翻译为 “逆文本规范化” 或 “反标准化”。它是语音识别后处理阶段中的一环，用于将识别结果从“读出来的字词”转换成更规范、更符合书写习惯的格式。\n\n参数值：\n\n-   true：开启；\n    \n-   false：不开启。\n    \n\n**enable\\_lid** `*boolean*`（可选）默认值为`false`\n\n是否在识别结果中返回语种识别信息。启用该能力后，识别结果中将包含语种信息。若通过`language`参数指定了待识别语种，返回结果中的语种值将与所指定的参数值一致。\n\n参数值：\n\n-   true：开启；\n    \n-   false：不开启。\n    \n\n**X-DashScope-DataInspection** `*string*` （可选）\n\n在通义千问 API 的内容安全能力基础上，是否进一步识别输入输出内容的违规信息。取值如下：\n\n-   `'{\"input\":\"cip\",\"output\":\"cip\"}'`：进一步识别；\n    \n-   不设置该参数：不进一步识别。\n    \n\n通过 HTTP 调用时请放入请求头：`-H \"X-DashScope-DataInspection: {\\\"input\\\": \\\"cip\\\", \\\"output\\\": \\\"cip\\\"}\"`；\n\n通过 Python SDK 调用时请通过`headers`配置：`headers={'X-DashScope-DataInspection': '{\"input\":\"cip\",\"output\":\"cip\"}'}`。\n\n详细使用方法请参见[内容审核](https://help.aliyun.com/zh/model-studio/content-security)。\n\n> 不支持通过 Java SDK 设置。\n\n> 不适用于 Qwen-VL、Qwen-Audio 系列模型。\n\n### chat响应对象（流式与非流式输出格式一致）\n\n```\n{\n  \"status_code\": 200,\n  \"request_id\": \"902fee3b-f7f0-9a8c-96a1-6b4ea25af114\",\n  \"code\": \"\",\n  \"message\": \"\",\n  \"output\": {\n    \"text\": null,\n    \"finish_reason\": null,\n    \"choices\": [\n      {\n        \"finish_reason\": \"stop\",\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"我是阿里云开发的一款超大规模语言模型，我叫通义千问。\"\n        }\n      }\n    ]\n  },\n  \"usage\": {\n    \"input_tokens\": 22,\n    \"output_tokens\": 17,\n    \"total_tokens\": 39\n  }\n}\n```\n\n**status\\_code** `*string*`\n\n本次请求的状态码。200 表示请求成功，否则表示请求失败。\n\n> Java SDK不会返回该参数。调用失败会抛出异常，异常信息为**status\\_code**和**message**的内容。\n\n**request\\_id** `*string*`\n\n本次调用的唯一标识符。\n\n> Java SDK返回参数为**requestId。**\n\n**code** `*string*`\n\n错误码，调用成功时为空值。\n\n> 只有Python SDK返回该参数。\n\n**output** `*object*`\n\n调用结果信息。\n\n**属性**\n\n**text** `*string*`\n\n模型生成的回复。当设置输入参数**result\\_format**为**text**时将回复内容返回到该字段。\n\n**finish\\_reason** `*string*`\n\n当设置输入参数**result\\_format**为**text**时该参数不为空。\n\n有四种情况：\n\n-   正在生成时为null；\n    \n-   因模型输出自然结束，或触发输入参数中的stop条件而结束时为stop；\n    \n-   因生成长度过长而结束为length；\n    \n-   因发生工具调用为tool\\_calls。\n    \n\n**choices** `*array*`\n\n模型的输出信息。当result\\_format为message时返回choices参数。\n\n**属性**\n\n**finish\\_reason** `*string*`\n\n有四种情况：\n\n-   正在生成时为null；\n    \n-   因模型输出自然结束，或触发输入参数中的stop条件而结束时为stop；\n    \n-   因生成长度过长而结束为length；\n    \n-   因发生工具调用为tool\\_calls。\n    \n\n**message** `*object*`\n\n模型输出的消息对象。\n\n**属性**\n\n**phase** `*string*`\n\n仅当调用通义千问深入研究模型 `qwen-deep-research`时，表示研究任务所处的阶段\n\n-   \"ResearchPlanning\"：研究规划阶段，执行计划内容存放在 [content](#48488e5b2czf9) 字段。\n    \n-   \"WebResearch\"：网络搜索阶段，搜索内容存放在 extra.deep\\_research.research 字段。\n    \n-   \"KeepAlive\"：维持流式连接，表明程序正在运行，不包含有用内容。\n    \n-   \"answer\"：回答阶段，研究内容存放在 content 字段。\n    \n\n**role** `*string*`\n\n输出消息的角色，固定为assistant。\n\n**content** `*string或array*`\n\n输出消息的内容。当使用qwen-vl或qwen-audio系列模型时为`array`，其余情况为`string`。\n\n> 如果发起Function Calling，则该值为空。\n\n**属性**\n\n**text** `*string*`\n\n当使用qwen-vl或qwen-audio系列模型时，输出消息的内容。\n\n当使用[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)时，输出语音识别结果。\n\n**image\\_hw** `*array*`\n\n当Qwen-VL系列模型启用 vl\\_enable\\_image\\_hw\\_output 参数时，有两种情况：\n\n-   图像输入：返回图像的高度和高度（数值单位：像素）\n    \n-   视频输入：返回空数组\n    \n\n**ocr\\_result** `*array*`\n\n当Qwen-OCR系列模型调用内置的信息抽取、高精识别任务时，输出的任务结果信息。\n\n**属性**\n\n**kv\\_result** `*array*`\n\n信息抽取任务的输出结果。\n\n**words\\_info** `*array*`\n\n高精识别任务的输出结果。\n\n**属性**\n\n**rotate\\_rect** `*array*`\n\n示例值：`[center_x, center_y, width, height, angle]`\n\n文字框的的旋转矩形表示：\n\n-   `center_x、center_y为文本框中心点坐标`\n    \n-   `width`为文本框宽度，`hight`为高度\n    \n-   `angle`为文本框相对于水平方向的旋转角度，取值范围为`[-90, 90]`\n    \n\n**location** `*array*`\n\n示例值：`[x1, y1, x2, y2, x3, y3, x4, y4]`\n\n文字框四个顶点的坐标，坐标顺序为左上角开起，按左上角→右上角→右下角→左下角的顺时针顺序排列。\n\n**text** `*string*`\n\n文本行的内容\n\n**annotations** `*array*`\n\n当使用[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)时，输出标注信息（如语种）\n\n**属性**\n\n**language** `*string*`\n\n被识别音频的语种。当请求参数`language`已指定语种时，该值与所指定的参数一致。\n\n可能的值如下：\n\n-   zh：中文\n    \n-   en：英文\n    \n-   ja：日语\n    \n-   de：德语\n    \n-   ko：韩语\n    \n-   ru：俄语\n    \n-   fr：法语\n    \n-   pt：葡萄牙语\n    \n-   ar：阿拉伯语\n    \n-   it：意大利语\n    \n-   es：西班牙语\n    \n\n**type** `*string*`\n\n固定为`audio_info`，表示音频信息。\n\n**extra** `*dict*`\n\n仅当调用通义千问深入研究模型 `qwen-deep-research`时，表示研究任务的附加信息。\n\n**属性**\n\n**deep\\_research** `*array*`\n\n深入研究相关信息\n\n**属性**\n\n**research** `*object*`\n\n研究任务信息\n\n**属性**\n\n**id** `*int*`\n\n研究任务ID\n\n**webSites** `*array*`\n\n搜索到的网站信息（仅在 status: \"streamingWebResult\" 时存在）\n\n**属性**\n\n**title** `*string*`\n\n网站标题\n\n**description** `*string*`\n\n网站描述\n\n**url** `*string*`\n\n网站url\n\n**favicon** `*string*`\n\n网站图标\n\n**learningMap** `*object*`\n\n模型从调用工具总结获取到的内容\n\n**reference** `*object*`\n\n参考文献信息（answer阶段，最终报告生成时）\n\n**属性**\n\n**icon** `*string*`\n\n网站图标\n\n**index\\_number** `*integer*`\n\n引用编号\n\n**description** `*string*`\n\n文献描述\n\n**title** `*string*`\n\n文献标题\n\n**url** `*string*`\n\n文献链接\n\n**status** `*string*`\n\n仅当调用通义千问深入研究模型 qwen-deep-research 时，表示当前阶段的任务执行状态。\n\n-   \"typing\"：模型正在工作，内容正在生成中\n    \n-   \"finished\"：当前阶段完成\n    \n-   \"streamingQueries\"：正在生成研究目标和搜索查询（WebResearch阶段）\n    \n-   \"streamingWebResult\"：正在执行搜索、网页阅读和代码执行（WebResearch阶段）\n    \n-   \"WebResultFinished\"：网络搜索阶段完成（WebResearch阶段）\n    \n\n**reasoning\\_content** `*string*`\n\nQwen3、[QwQ 模型](https://help.aliyun.com/zh/model-studio/deep-thinking)、[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)的深度思考内容。\n\n**tool\\_calls** `*array*`\n\n如果模型需要调用工具，则会生成tool\\_calls参数。\n\n**属性**\n\n**function** `object`\n\n调用工具的名称，以及输入参数。\n\n**属性**\n\n**name** `*string*`\n\n调用工具的名称\n\n**arguments** `*string*`\n\n需要输入到工具中的参数，为JSON字符串。\n\n> 由于大模型响应有一定随机性，输出的JSON字符串并不总满足于您的函数，建议您在将参数输入函数前进行参数的有效性校验。\n\n**index** `*integer*`\n\n当前**tool\\_calls**对象在tool\\_calls数组中的索引。\n\n**id** `*string*`\n\n本次工具响应的ID。\n\n**type** `*string*`\n\n工具类型，固定为`function`。\n\n**logprobs** `*object*`\n\n当前 choices 对象的概率信息。\n\n**属性**\n\n**content** `*array*`\n\n带有对数概率信息的 Token 数组。\n\n**属性**\n\n**token** `*string*`\n\n当前 Token。\n\n**bytes** `*array*`\n\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\n\n**logprob** `*float*`\n\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\n\n**top\\_logprobs** `*array*`\n\n当前 Token 位置最可能的若干个 Token 及其对数概率，元素个数与入参的`top_logprobs`保持一致。\n\n**属性**\n\n**token** `*string*`\n\n当前 Token。\n\n**bytes** `*array*`\n\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\n\n**logprob** `*float*`\n\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\n\n**search\\_info** `*object*`\n\n联网搜索到的信息，在设置`search_options`参数后会返回该参数。\n\n**属性**\n\n**search\\_results** `*array*`\n\n联网搜索到的结果。\n\n**属性**\n\n**site\\_name** `*string*`\n\n搜索结果来源的网站名称。\n\n**icon** `*string*`\n\n来源网站的图标URL，如果没有图标则为空字符串。\n\n**index** `*integer*`\n\n搜索结果的序号，表示该搜索结果在`search_results`中的索引。\n\n**title** `*string*`\n\n搜索结果的标题。\n\n**url** `*string*`\n\n搜索结果的链接地址。\n\n**extra\\_tool\\_info** `*array*`\n\n开启`enable_search_extension`参数后返回的领域增强信息。\n\n**属性**\n\n**result** `*string*`\n\n领域增强工具输出信息。\n\n**tool** `*string*`\n\n领域增强使用的工具。\n\n**usage** `*map*`\n\n本次chat请求使用的Token信息。\n\n**属性**\n\n**input\\_tokens** `*integer*`\n\n用户输入内容转换成Token后的长度。\n\n**output\\_tokens** `*integer*`\n\n模型输出内容转换成Token后的长度。\n\n**input\\_tokens\\_details** `*integer*`\n\n使用[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)、[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)或[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)时，输入内容转换成Token后的长度详情。\n\n**属性**\n\n**text\\_tokens** `*integer*`\n\n使用[Qwen-VL 模型](https://help.aliyun.com/zh/model-studio/vision)或[QVQ模型](https://help.aliyun.com/zh/model-studio/qvq)时，为输入的文本转换为Token后的长度。\n\n使用[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)时，为使用上下文增强功能时输入的文本长度，上限为10000 Token。\n\n**image\\_tokens** `*integer*`\n\n输入的图像转换为Token后的长度。\n\n**video\\_tokens** `*integer*`\n\n输入的视频文件或图像列表转换为Token后的长度。\n\n**total\\_tokens** `*integer*`\n\n当输入为纯文本时返回该字段，为**input\\_tokens**与**output\\_tokens**之和**。**\n\n**image\\_tokens** `*integer*`\n\n输入内容包含`image`时返回该字段。为用户输入图片内容转换成Token后的长度。\n\n**video\\_tokens** `*integer*`\n\n输入内容包含`video`时返回该字段。为用户输入视频内容转换成Token后的长度。\n\n**audio\\_tokens** `*integer*`\n\n输入内容包含`audio`时返回该字段。为用户输入音频内容转换成Token后的长度。\n\n**output\\_tokens\\_details** `*integer*`\n\n输出内容转换成 Token后的长度详情。\n\n**属性**\n\n**text\\_tokens** `*integer*`\n\n输出的文本转换为Token后的长度。\n\n**reasoning\\_tokens** `*integer*`\n\nQwen3 模型思考过程转换为Token后的长度。\n\n**prompt\\_tokens\\_details** `*object*`\n\n输入 Token 的细粒度分类。\n\n**属性**\n\n**cached\\_tokens** `*integer*`\n\n命中 Cache 的 Token 数。Context Cache 详情请参见[上下文缓存](https://help.aliyun.com/zh/model-studio/context-cache)。\n\n**cache\\_creation** `*object*`\n\n[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)创建信息。\n\n**属性**\n\n**ephemeral\\_5m\\_input\\_tokens** `*integer*`\n\n用于创建5分钟有效期显式缓存的 Token 长度。\n\n**cache\\_creation\\_input\\_tokens** `*integer*`\n\n用于创建显式缓存的 Token 长度。\n\n**cache\\_type** `*string*`\n\n使用[显式缓存](https://help.aliyun.com/zh/model-studio/context-cache#825f201c5fy6o)时，参数值为`ephemeral`，否则该参数不存在。\n\n**seconds** `*integer*`\n\n使用[录音文件识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-speech-recognition)时，为音频时长（单位为秒）。\n\n## **错误码**\n\n如果模型调用失败并返回报错信息，请参见[错误信息](https://help.aliyun.com/zh/model-studio/error-code)进行解决。\n\n  \n\n   \n\n[上一篇：对话](/zh/model-studio/chat/)[下一篇：DeepSeek](/zh/model-studio/deepseek-api)\n\n该文章对您有帮助吗？\n\n反馈\n\n### 为什么选择阿里云\n\n[什么是云计算](https://www.aliyun.com/about/what-is-cloud-computing)[全球基础设施](https://infrastructure.aliyun.com/)[技术领先](https://www.aliyun.com/why-us/leading-technology)[稳定可靠](https://www.aliyun.com/why-us/reliability)[安全合规](https://www.aliyun.com/why-us/security-compliance)[分析师报告](https://www.aliyun.com/analyst-reports)\n\n### 产品和定价\n\n[全部产品](https://www.aliyun.com/product/list)[免费试用](https://free.aliyun.com/)[产品动态](https://www.aliyun.com/product/news/)[产品定价](https://www.aliyun.com/price/detail)[配置报价器](https://www.aliyun.com/price/cpq/list)[云上成本管理](https://www.aliyun.com/price/cost-management)\n\n### 解决方案\n\n[技术解决方案](https://www.aliyun.com/solution/tech-solution)\n\n### 文档与社区\n\n[文档](https://help.aliyun.com/)[开发者社区](https://developer.aliyun.com/)[天池大赛](https://tianchi.aliyun.com/)[培训与认证](https://edu.aliyun.com/)\n\n### 权益中心\n\n[免费试用](https://free.aliyun.com/)[解决方案免费试用](https://www.aliyun.com/solution/free)[高校计划](https://university.aliyun.com/)[5亿算力补贴](https://www.aliyun.com/benefit/form/index)[推荐返现计划](https://dashi.aliyun.com/?ambRef=shouYeDaoHang2&pageCode=yunparterIndex)\n\n### 支持与服务\n\n[基础服务](https://www.aliyun.com/service)[企业增值服务](https://www.aliyun.com/service/supportplans)[迁云服务](https://www.aliyun.com/service/devopsimpl/devopsimpl_cloudmigration_public_cn)[官网公告](https://www.aliyun.com/notice/)[健康看板](https://status.aliyun.com/)[信任中心](https://security.aliyun.com/trust)\n\n### 关注阿里云\n\n关注阿里云公众号或下载阿里云APP，关注云资讯，随时随地运维管控云服务\n\n![阿里云APP](https://img.alicdn.com/imgextra/i4/O1CN01XLesV31fkf7pYNATb_!!6000000004045-2-tps-400-400.png)![阿里云微信](https://img.alicdn.com/tfs/TB1AOdINW6qK1RjSZFmXXX0PFXa-258-258.jpg)\n\n联系我们：4008013260\n\n[法律声明](https://help.aliyun.com/product/67275.html)[Cookies政策](https://terms.alicdn.com/legal-agreement/terms/platform_service/20220906101446934/20220906101446934.html)[廉正举报](https://aliyun.jubao.alibaba.com/)[安全举报](https://report.aliyun.com/)[联系我们](https://www.aliyun.com/contact)[加入我们](https://careers.aliyun.com/)\n\n### 友情链接\n\n[阿里巴巴集团](https://www.alibabagroup.com/cn/global/home)[淘宝网](https://www.taobao.com/)[天猫](https://www.tmall.com/)[全球速卖通](https://www.aliexpress.com/)[阿里巴巴国际交易市场](https://www.alibaba.com/)[1688](https://www.1688.com/)[阿里妈妈](https://www.alimama.com/index.htm)[飞猪](https://www.fliggy.com/)[阿里云计算](https://www.aliyun.com/)[AliOS](https://www.alios.cn/)[万网](https://wanwang.aliyun.com/)[高德](https://mobile.amap.com/)[UC](https://www.uc.cn/)[友盟](https://www.umeng.com/)[优酷](https://www.youku.com/)[钉钉](https://www.dingtalk.com/)[支付宝](https://www.alipay.com/)[达摩院](https://damo.alibaba.com/)[淘宝海外](https://world.taobao.com/)[阿里云盘](https://www.aliyundrive.com/)[饿了么](https://www.ele.me/)\n\n© 2009-2025 Aliyun.com 版权所有 增值电信业务经营许可证： [浙B2-20080101](http://beian.miit.gov.cn/) 域名注册服务机构许可： [浙D3-20210002](https://domain.miit.gov.cn/域名注册服务机构/互联网域名/阿里云计算有限公司 )\n\n[![](//gw.alicdn.com/tfs/TB1GxwdSXXXXXa.aXXXXXXXXXXX-65-70.gif)](https://zzlz.gsxt.gov.cn/businessCheck/verifKey.do?showType=p&serial=91330106673959654P-SAIC_SHOW_10000091330106673959654P1710919400712&signData=MEUCIQDEkCd8cK7%2Fyqe6BNMWvoMPtAnsgKa7FZetfPkjZMsvhAIgOX1G9YC6FKyndE7o7hL0KaBVn4f%20V%2Fiof3iAgpsV09o%3D)[![浙公网安备 33010602009975号](//img.alicdn.com/tfs/TB1..50QpXXXXX7XpXXXXXXXXXX-40-40.png)浙公网安备 33010602009975号](http://www.beian.gov.cn/portal/registerSystemInfo)[浙B2-20080101-4](https://beian.miit.gov.cn/)",
    "metadata": {},
    "title": "https://help.aliyun.com/zh/dashscope/developer-reference/api-details (text/html;charset=UTF-8)",
    "time": {
      "start": 1759400827938,
      "end": 1759400831129
    }
  }
}
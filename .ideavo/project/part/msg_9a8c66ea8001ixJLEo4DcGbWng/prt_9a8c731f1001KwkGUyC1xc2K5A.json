{
  "id": "prt_9a8c731f1001KwkGUyC1xc2K5A",
  "messageID": "msg_9a8c66ea8001ixJLEo4DcGbWng",
  "sessionID": "ses_65b8a18f2ffeMlgQ4UP0yHPHsm",
  "type": "tool",
  "tool": "write",
  "callID": "toolu_01UHTVTtRM7bM56QuozFNTBw",
  "state": {
    "status": "completed",
    "input": {
      "filePath": "/home/user/project/app/api/scrape-website/route.ts",
      "content": "import { NextRequest, NextResponse } from 'next/server'\nimport puppeteer from 'puppeteer'\nimport * as cheerio from 'cheerio'\nimport { JSDOM } from 'jsdom'\nimport { Readability } from '@mozilla/readability'\nimport validator from 'validator'\nimport { z } from 'zod'\n\n// Input validation schema\nconst ScrapeRequestSchema = z.object({\n  url: z.string().url('Invalid URL format'),\n  extractMethod: z.enum(['readability', 'basic']).default('readability'),\n  timeout: z.number().min(1000).max(30000).default(10000)\n})\n\n// Rate limiting (simple in-memory store for demo)\nconst rateLimitStore = new Map<string, { count: number; resetTime: number }>()\nconst RATE_LIMIT = 10 // requests per minute\nconst RATE_LIMIT_WINDOW = 60 * 1000 // 1 minute\n\nfunction checkRateLimit(ip: string): boolean {\n  const now = Date.now()\n  const record = rateLimitStore.get(ip)\n  \n  if (!record || now > record.resetTime) {\n    rateLimitStore.set(ip, { count: 1, resetTime: now + RATE_LIMIT_WINDOW })\n    return true\n  }\n  \n  if (record.count >= RATE_LIMIT) {\n    return false\n  }\n  \n  record.count++\n  return true\n}\n\nasync function scrapeWithPuppeteer(url: string, timeout: number): Promise<string> {\n  let browser = null\n  \n  try {\n    browser = await puppeteer.launch({\n      headless: 'new',\n      args: [\n        '--no-sandbox',\n        '--disable-setuid-sandbox',\n        '--disable-dev-shm-usage',\n        '--disable-gpu',\n        '--no-first-run',\n        '--no-zygote',\n        '--single-process'\n      ]\n    })\n    \n    const page = await browser.newPage()\n    \n    // Set user agent and viewport\n    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n    await page.setViewport({ width: 1280, height: 720 })\n    \n    // Navigate with timeout\n    await page.goto(url, { \n      waitUntil: 'domcontentloaded',\n      timeout \n    })\n    \n    // Wait for content to load\n    await page.waitForTimeout(2000)\n    \n    // Get the HTML content\n    const html = await page.content()\n    return html\n    \n  } finally {\n    if (browser) {\n      await browser.close()\n    }\n  }\n}\n\nfunction extractContentWithReadability(html: string, url: string): { title: string; content: string; excerpt: string } {\n  const dom = new JSDOM(html, { url })\n  const reader = new Readability(dom.window.document)\n  const article = reader.parse()\n  \n  if (!article) {\n    throw new Error('Could not extract readable content from the page')\n  }\n  \n  return {\n    title: article.title || 'Untitled',\n    content: article.textContent || '',\n    excerpt: article.excerpt || ''\n  }\n}\n\nfunction extractContentBasic(html: string): { title: string; content: string; excerpt: string } {\n  const $ = cheerio.load(html)\n  \n  // Remove script and style elements\n  $('script, style, nav, header, footer, aside').remove()\n  \n  // Extract title\n  const title = $('title').text() || $('h1').first().text() || 'Untitled'\n  \n  // Extract main content\n  let content = ''\n  const contentSelectors = [\n    'main',\n    'article',\n    '[role=\"main\"]',\n    '.content',\n    '.main-content',\n    '#content',\n    '#main'\n  ]\n  \n  for (const selector of contentSelectors) {\n    const element = $(selector)\n    if (element.length > 0) {\n      content = element.text().trim()\n      break\n    }\n  }\n  \n  // Fallback to body if no main content found\n  if (!content) {\n    content = $('body').text().trim()\n  }\n  \n  // Clean up content\n  content = content\n    .replace(/\\s+/g, ' ')\n    .replace(/\\n+/g, '\\n')\n    .trim()\n  \n  const excerpt = content.length > 200 ? content.substring(0, 200) + '...' : content\n  \n  return { title, content, excerpt }\n}\n\nexport async function POST(request: NextRequest) {\n  try {\n    // Get client IP for rate limiting\n    const clientIP = request.headers.get('x-forwarded-for')?.split(',')[0] || \n                     request.headers.get('x-real-ip') || \n                     'unknown'\n    \n    // Check rate limit\n    if (!checkRateLimit(clientIP)) {\n      return NextResponse.json(\n        { error: 'Rate limit exceeded. Please try again later.' },\n        { status: 429 }\n      )\n    }\n    \n    const body = await request.json()\n    \n    // Validate input\n    const validatedInput = ScrapeRequestSchema.parse(body)\n    const { url, extractMethod, timeout } = validatedInput\n    \n    // Additional URL validation\n    if (!validator.isURL(url, { \n      protocols: ['http', 'https'], \n      require_protocol: true \n    })) {\n      return NextResponse.json(\n        { error: 'Invalid URL. Please provide a valid HTTP or HTTPS URL.' },\n        { status: 400 }\n      )\n    }\n    \n    console.log(`Scraping website: ${url} using ${extractMethod} method`)\n    \n    // Scrape the website\n    const html = await scrapeWithPuppeteer(url, timeout)\n    \n    if (!html || html.length < 100) {\n      throw new Error('No content found or page too small')\n    }\n    \n    // Extract content using the specified method\n    let extractedContent\n    \n    if (extractMethod === 'readability') {\n      extractedContent = extractContentWithReadability(html, url)\n    } else {\n      extractedContent = extractContentBasic(html)\n    }\n    \n    // Limit content size for translation\n    const maxContentLength = 5000\n    if (extractedContent.content.length > maxContentLength) {\n      extractedContent.content = extractedContent.content.substring(0, maxContentLength) + '...'\n    }\n    \n    return NextResponse.json({\n      success: true,\n      url,\n      title: extractedContent.title,\n      content: extractedContent.content,\n      excerpt: extractedContent.excerpt,\n      contentLength: extractedContent.content.length,\n      extractMethod\n    })\n    \n  } catch (error) {\n    console.error('Website scraping error:', error)\n    \n    if (error instanceof z.ZodError) {\n      return NextResponse.json(\n        { error: 'Invalid input', details: error.errors },\n        { status: 400 }\n      )\n    }\n    \n    if (error instanceof Error) {\n      if (error.message.includes('timeout')) {\n        return NextResponse.json(\n          { error: 'Website took too long to load. Please try again.' },\n          { status: 408 }\n        )\n      }\n      \n      if (error.message.includes('net::ERR')) {\n        return NextResponse.json(\n          { error: 'Could not connect to the website. Please check the URL.' },\n          { status: 400 }\n        )\n      }\n    }\n    \n    return NextResponse.json(\n      { error: 'Failed to scrape website. Please try again later.' },\n      { status: 500 }\n    )\n  }\n}"
    },
    "output": "",
    "metadata": {
      "diagnostics": {},
      "filepath": "/home/user/project/app/api/scrape-website/route.ts",
      "exists": false
    },
    "title": "app/api/scrape-website/route.ts",
    "time": {
      "start": 1759473274403,
      "end": 1759473274406
    }
  }
}